{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import umap\n",
    "from dotenv import load_dotenv\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def safe_literal_eval(val):\n",
    "    if pd.isna(val):  \n",
    "        return []     \n",
    "    try:\n",
    "        return ast.literal_eval(val)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return [] \n",
    "    \n",
    "def clean_text(text):\n",
    "    # quotes\n",
    "    text = re.sub(r\"(?m)^\\s*>.*(?:\\r?\\n|$)\", \"\", text)\n",
    "\n",
    "    # links and code\n",
    "    # should we consider using LLM to generate short summary of the code snippets so we don't lose any context it could provide\n",
    "    # or could use regex to detect patterns in the code and classify them e.g. detecting import statements, function definitions, or added/removed lines.\n",
    "    pattern = r\"```.*?```|http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "    cleaned_text = re.sub(pattern, \"\", text, flags=re.DOTALL)\n",
    "\n",
    "    # keep only alphanumeric characters and punctuation\n",
    "    cleaned_text = re.sub(r\"[^a-zA-Z0-9.,!?;:'\\\"(){}\\[\\]\\-]\", \" \", cleaned_text)\n",
    "\n",
    "    # remove extra spaces\n",
    "    cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()\n",
    "\n",
    "    return cleaned_text\n",
    "    \n",
    "def preprocess_text(comments_sequence):\n",
    "    all_threads = []\n",
    "    for comment_thread in comments_sequence:\n",
    "        main_comment = comment_thread['comment']['body']\n",
    "        replies = [reply['body'] for reply in comment_thread['replies']]\n",
    "        thread = main_comment + \"\\n\" + \"\\n\".join(replies)\n",
    "        \n",
    "        thread = clean_text(thread)\n",
    "        if thread != \"\":\n",
    "            all_threads.append(thread)\n",
    "\n",
    "    return all_threads\n",
    "\n",
    "df= pd.read_csv('data/pull_requests_filtered_raw.csv')\n",
    "df['comments'] = df['comments'].apply(safe_literal_eval)\n",
    "\n",
    "df['review_threads'] = df['comments'].apply(lambda comments: [item for item in comments if item['type'] == 'review'] if type(comments) is not float else comments)\n",
    "df = df[df['review_threads'].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "df['processed_review_threads'] = df['review_threads'].apply(preprocess_text)\n",
    "\n",
    "threads = sum(df['processed_review_threads'].tolist(), [])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Using Bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://bertopic.readthedocs.io/en/latest/\n",
    "# https://maartengr.github.io/BERTopic/api/representation/keybert.html\n",
    "# https://maartengr.github.io/BERTopic/getting_started/vectorizers/vectorizers.html\n",
    "\n",
    "# used to fine-tune topic representations\n",
    "representation_model = KeyBERTInspired()\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "umap_model = umap.UMAP(n_neighbors=15, n_components=5, random_state=42)     # fix topics across runs by setting random_state; otherwise UMAP is stochastic\n",
    "\n",
    "# using pre-calculated embeddings\n",
    "# topic_model = BERTopic(min_topic_size=20)\n",
    "# topics, probs = topic_model.fit_transform(threads, thread_embeddings)\n",
    "\n",
    "# using KeyBERTInspired to generate embeddings\n",
    "topic_model = BERTopic(representation_model=representation_model, vectorizer_model=vectorizer_model, umap_model=umap_model, min_topic_size=20)\n",
    "topics, probs = topic_model.fit_transform(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Topic Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Pandas formatting for printing\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_model.get_topic_info())\n",
    "\n",
    "# topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://maartengr.github.io/BERTopic/getting_started/hierarchicaltopics/hierarchicaltopics.html\n",
    "\n",
    "# topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n",
    "hierarchical_topics = topic_model.hierarchical_topics(threads)\n",
    "print(topic_model.get_topic_tree(hierarchical_topics))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 38\n",
    "topics_df = pd.DataFrame({'topic': topics, 'document': threads})\n",
    "\n",
    "# imports_cluster = [38, 75]\n",
    "\n",
    "# +from .. import ads (from reviewer) [2024-09-10T13:50:17Z] Don't do this, rather just import what you need    NOTE: should use relative within component code and \n",
    "# importing private methods\n",
    "\n",
    "\n",
    "print(topic_model.get_topic_info(topic))\n",
    "print(topics_df[topics_df.topic == topic])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to get info about specific topics in hierarchy\n",
    "# print(topic_model.get_representative_docs(65))\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "topics_df = pd.DataFrame({'topic': topics, 'document': threads})\n",
    "print(len(topics_df[topics_df.topic == 13]))\n",
    "print(topics_df[topics_df.topic == 13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typing_casting_cluster = [139, 36, 123, 107, 53, 105, 87]        \n",
    "redundant_code_comments_cluster = [85, 153, 113, 167, 15, 106, 181, 148, 47, 138, 63, 126, 86, 177, 55, 172, 156, 185, 96]\n",
    "fixed_cluster = [132, 180]      # 'Fixed in ...'\n",
    "code_style_cluster = [128, 176, 130, 110, 52, 91, 19, 98, 30, 103, 109, 13, 42, 70, 89, 127, 114, 99, 170, 79, 144, 166, 168, 71, 76, 21, 90, 59]       # indentation, if statement styles, moving checks, single-use variables, renaming, using const, linting\n",
    "thanks_acknowledgements_cluster = [39, 46]\n",
    "process_cluster = [162, 12, 45, 73, 56]      # dependency bumps in different PR, \"do in another PR\", \"revert change/not needed\", rebasing, limit to one platform per PR\n",
    "misc_cluster = [152, 175, 82, 173, 97, 74, 152, 37]  # \"same here\", \"done\", \"same as above\" etc\n",
    "imports_cluster = [38, 75]  # e.g. https://github.com/home-assistant/core/pull/77091#discussion_r950871893 and https://github.com/home-assistant/core/pull/49116#discussion_r611703022\n",
    "config_entries_cluster = [145, 149, 77, 83, 101, 140, 188, 187, 17, 9, 88, 163, 44, 35]     # including move away from YAML\n",
    "async_eventloop_cluster = [81, 48, 64, 125, 34]\n",
    "error_handling_cluster = [104, 72, 151, 40, 24, 150, 124]\n",
    "logging_cluster = [122, 60, 31, 131]\n",
    "measurements_cluster = [29, 129, 157]    # unit of measurements, sensor measurements, power/watts/energy\n",
    "icon_name_device_translations_cluster = [14, 67, 4]\n",
    "time_timezone_dates_duration_cluster = [16, 95]\n",
    "classes_inheritance_attributes_cluster = [23, 171, 142]\n",
    "dictionary_dictkeys_cluster = [43, 3, 158]\n",
    "update_coordinator_interval_polling_cluster = [18, 116, 93, 6]\n",
    "testing_cluster = [119, 174, 32, 51, 11, 160, 94, 178, 111, 102, 182]\n",
    "external_library_cluster = [136, 133, 57]   # this is also present partially in other clusters\n",
    "unique_ids_cluster = [65, 33]\n",
    "assigning_attributes_cluster = [143, 27, 92, 80, 179]\n",
    "entity_cluster = [1, 164, 137, 118]\n",
    "protocols_cluster = [165, 66, 115, 134, 146, 61, 68]\n",
    "device_cluster = [25, 49]\n",
    "validation_schemas_cluster = [155, 78, 28]\n",
    "blueprints_cluster = [135]      # use a select selector for blueprint\n",
    "state_cluster = [7, 169, 100]\n",
    "domain_cluster = [141]\n",
    "service_cluster = [54]\n",
    "authentication_cluster = [2]\n",
    "voice_assistant_conversation_cluster = [154, 159]\n",
    "questions_cluster = [121]\n",
    "other_cluster = [186, 120, 161, 147, 117, 20, 41, 184, 147, 183, *fixed_cluster, *misc_cluster, *blueprints_cluster]\n",
    "\n",
    "# remaining topics included in other:\n",
    "# 183 = Tuya (brand) devices\n",
    "# 184 = caching, cache decorator\n",
    "# 186 = comments about tilt, cover, ... of blinds- basically all from https://github.com/home-assistant/core/pull/48625\n",
    "# 120 = random number string noise\n",
    "# 147 = minValue, maxValue, min max\n",
    "# 161 = mixins\n",
    "# 117 = general \"update\" stuff\n",
    "# 20 = bunch of mixed things involving Home Assistant\n",
    "# 41 = mixed bag of integration related items\n",
    "\n",
    "# remaining topics in own cluster\n",
    "# 8 = light, rbg, brightness\n",
    "# 5 = temperature, hvac, fans, climate\n",
    "# 10 = defaults\n",
    "# 0 = sensor entities, sensors\n",
    "# 112 = humidity, humidifier integration\n",
    "# 62 = locks, alarms integrations\n",
    "# 84 = images, cameras\n",
    "# 22 = hass object, hass.data etc\n",
    "# 26 = media players\n",
    "# 58 = notifications, notification integration\n",
    "# 69 = buttons, button integration\n",
    "\n",
    "\n",
    "topics_to_merge = [typing_casting_cluster, redundant_code_comments_cluster, code_style_cluster, thanks_acknowledgements_cluster, process_cluster, imports_cluster, config_entries_cluster, async_eventloop_cluster,\n",
    "                   error_handling_cluster, logging_cluster, measurements_cluster, icon_name_device_translations_cluster, time_timezone_dates_duration_cluster, classes_inheritance_attributes_cluster, validation_schemas_cluster, dictionary_dictkeys_cluster, update_coordinator_interval_polling_cluster,\n",
    "                   testing_cluster, external_library_cluster, unique_ids_cluster, assigning_attributes_cluster, entity_cluster, protocols_cluster, device_cluster, validation_schemas_cluster, state_cluster, domain_cluster, service_cluster,\n",
    "                   authentication_cluster, voice_assistant_conversation_cluster, questions_cluster, other_cluster]\n",
    "\n",
    "# merge topics and re-assign topics to input data\n",
    "topic_model.merge_topics(threads, topics_to_merge)\n",
    "topics, probs = topic_model.transform(threads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updated Topic Info and Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_topics = topic_model.hierarchical_topics(threads)\n",
    "print(topic_model.get_topic_tree(hierarchical_topics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get updated info about specific clusters\n",
    "print(topic_model.get_representative_docs(61))\n",
    "\n",
    "topics_df = pd.DataFrame({'topic': topics, 'document': threads})\n",
    "print(topics_df[topics_df.topic == 61])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of noise comments and not-noise\n",
    "topic_counts = topic_model.get_topic_info()\n",
    "noise_counts = topic_counts[topic_counts[\"Topic\"] == -1][[\"Topic\", \"Count\"]]\n",
    "valid_counts = topic_counts[topic_counts[\"Topic\"] != -1][[\"Topic\", \"Count\"]]\n",
    "\n",
    "print('Noise: ', sum(noise_counts['Count']))\n",
    "print('Classified comments: ', sum(valid_counts['Count']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
