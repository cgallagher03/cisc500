Topic,Count,Name,Representation,Representative_Docs
-1,9054,-1_entity_entities_async_devices,"['entity', 'entities', 'async', 'devices', 'config', 'device', 'attribute', 'update', 'coordinator', 'id']","[""Move the objects that are being created here every loop outside the loop so you only create them once I don't get this one sorry. There is only a single thing updated and then the loop gets broken I pushed a suggestion. It might need to be adjusted, but hopefully its enough to get you there. Thank you. But I don't understand the benefit, sorry. What are we trying to achieve here? The goal is to not have to pay the cost of recreating the variables every loop since they are created outside the loop I'm not trying to argue with you here btw, just legitimately curious and trying to learn something, because for me it all looks the same... the only variable that is created is name and that still happens on every loop now machine self.config entry.data[CONF MACHINE] is moved out side the loop Let's unpack this one self.config entry is having to look up config entry every loop .. similar to getattr(self,'config entry') config entry.data is having to lookup data every loop .. similar to getattr(config entry,'data') CONF MACHINE has to be looked up in data every loop But assigning it to the local var machine it only has to lookup machine tuple(BT MODEL NAMES) has to create the tuple every loop You can disassemble the code with to look at what its really doing tyvm. Understood, though I would have thought those to be ignorable performance wise (but I know you're the HA performance god, so I wanted to take that chance ). Why are you negating the condition though and using continue ? For most cases it won't matter, but we have users with 2000 visible bluetooth devices (or more -- some people live near train stations, etc) so you could end up creating 2000 tuples every time it looped on the miss case. It makes the code more readable by saving indent kinda disagree on that last one, because the code itself gets harder to read imo (especially since that introduces an uncovered line, which I needed to add an extra test for ) and btw. by using and we can avoid doing the split on every loop because we can jump out when the prefix doesn't match (can't we) Its just a suggestion, if you don't like it feel free to revert it and do it another way done. Sometimes it's hard to tell which reviews are suggestions and which are mandatory from the reviewers side. As soon as one of the or is true the evaluation will stop, the continue will be reached so you'll never get to the split That's because the CI is measuring line cover, not branch coverage so the code is still uncovered even if you remove the guard. that's true of course, but tbh in this case I only care about line coverage, as there is no code to test for the other case yeah, right ofc"", ""Why do we still allow entities to be selected? Is the device alone not enough? You can always select entities when using the target, I am merely filtering. And I did make it work through entities or devices, both work. That keeps me happy since I can use the entity id :) And you can also use device id is you prefer. It's not up to each integration to add other ids than the required id, just for the sake of ease. That should be handled centrally by Home Assistant if we want to do that. Please remove the entity id selector. Sure I will remove it, but it does not make any sense to me. Then you get a target select in the UI in which the user can select devices or entities (or area's or even labels) and when the user selects a entity it will just not do anything. Nothing in the UI tell the user to use the device as target and not a entity as target. Seems very confusing to me. Edit: See post below, now makes sense entity selection has been removed"", ""I noticed the setup purpleair fixture uses async setup component and passes a config. That's not useful. This integration only uses config entries, so it's better to use hass.config entries.async setup to set up the config entry. But I don't always want the config entry to be set up automatically. Using hass.config entries.async setup will cause most of my tests to fail because they'll be attempting to duplicate an already-existing config entry. async setup component ensures that I can detach platform setup (including entities, devices, etc.) from where the config entry exists. I'm not following. hass.config entries.async setup will set up a config entry, it doesn't create a config entry. Where do I get that config entry ID from? From the existing config entry fixture? Yes. Ah, okay then, in my config flow tests, I can just make sure to use different unique data to avoid duplicates. Thank you!""]"
0,694,0_sensor_sensors_sensorentitydescription_sensorpy,"['sensor', 'sensors', 'sensorentitydescription', 'sensorpy', 'sensorentity', 'binary', 'alarm', 'switch', 'device', 'logic']","['Sensor should be in sensor platform.', 'So this sensor would just always be none?', 'Binary sensor?']"
1,432,1_entitydescription_entitydescriptions_entity_entities,"['entitydescription', 'entitydescriptions', 'entity', 'entities', 'entitynaming', 'descriptionkey', 'changingtheentitymodel', 'descriptions', 'description', 'property']","['You could use description instead of self.entity description here. Changed', 'Must use self.entity description here as entity description can be set also as class variable', 'Why not set the state class in the entity description?']"
2,416,2_reauth_reauthentication_oauth_invalidauth,"['reauth', 'reauthentication', 'oauth', 'invalidauth', 'auth', 'authentication', 'configentryauthfailed', 'authenticate', 'credentials', 'login']","[""If it's an authentication error we should probably raise ConfigEntryAuthFailed and add a reauth step in the config flow, to let the user re-authenticate. For other connection errors, that we think may be resolved automatically, we raise ConfigEntryNotReady ."", 'Please add reauth in a separate PR', 'In a future PR you can raise ConfigEntryAuthFailed to start a reauth flow. Reauth would be a nice addition in the future indeed in case a user updates their password :)']"
3,391,3_dict_dicts_dictget_dictgetkey,"['dict', 'dicts', 'dictget', 'dictgetkey', 'dictkey', 'dictionaries', 'dictstr', 'dictsetdefault', 'typeddict', 'dictionary']","['is this dict ever used ?', 'Why the dict', 'Should already be a dict?']"
4,379,4_translations_translators_translator_translated,"['translations', 'translators', 'translator', 'translated', 'translation', 'translate', 'scripttranslations', 'localize', 'locale', 'english']","[""Translations are handled by Lokalise. Please don't change any translation files, besides the default English translation that is autogenerated when running the translations develop script. I've just copied the translation files from transmission, since the same strings are used by both integrations. Reading the documentation, I'm not sure if I have to do something more to properly add the translations. The translations are provided by the translation community via Lokalise. We don't add translations here. It's the other way around. Please read the whole translations docs. Understood. Thanks."", ""Why is the name not translated? I just copied the the schedulepart VenstarSensorEntityDescription above and modified it for this. How would I make this a translatable string? You should set the translation key (and optionally also the translation placeholders ) of the entity description and update strings.json . For local testing, you need to run python3 -m script.translations develop --all for the (English) name to work. I already have translation key on this entity description and have the translations in strings.json . Is there anything else I need to do? The translations in strings.json only translate the states, there's no translation of the name."", 'We should not use the translation key for it. The translation key should only be used for translations.']"
5,347,5_thermostats_thermostat_hvacmodeoff_hvac,"['thermostats', 'thermostat', 'hvacmodeoff', 'hvac', 'temperatures', 'climate', 'temperature', 'modes', 'heating', 'heater']","['The auto mode doesn\'t support manual temperature control. The heat cool mode supports both low and high setpoints. In ha, are auto mode and heat cool mode the same? No. Auto mode means the user can\'t set the target temperature, while heat cool mode allows the user to set low and high target temperature setpoints. Both modes have some level of automatic control, but they are not the same. hvac-modes I couldn\'t find the set methods for low and high in the base class of climate L402 We use async set temperature to set those setpoints as well. Just look for the proper parameter names. Here\'s the service schema. L118-L130 Here we may need to wait for the write attribute of the matter server to be added to the dependency of HA before we can proceed, because the setpoint cannot be achieved through a command We actually added that yesterday based on your feedback so we could move on and add that in this beta too. So if I understand it correctly, some thermostats do support setting the hvac mode but you need to write the attribute yourself instead that there\'s is some ready made command within the cluster, right ? Yes, besides the system mode, when we need to adjust the low and high set points in the heat cool mode mentioned here, we also need to use write attribute to set them As soon as [this PR]( is merged, you can rebase your branch and you will have a ""write attribute"" available in the client.', 'The hvac mode must be one of the standard climate states of the climate entity design. hvac-modes Please create a map between the yolink device modes and the Home Assistant climate hvac modes.', 'The user should not choose HVAC mode, that is normally something the program does (or more correctly the automation connected). Some climate controls deliver the hvac mode, but there are no standard in modbus for that. That is correct - there is no ""standard"" for the hvac mode. I would argue the same is true for a modbus ""climate"" device in general, this entire integration doesn\'t really follow a standard. It\'s supposed to be generic enough that with enough configuration of registers, types, etc, that Home Assistant allows the device to be used as a thermostat. I want to add support for more thermostat features - as generic as possible. That being said - I spent quite a bit of time writing this to be as generic as possible, and to allow the user to map a device register to either the active mode HVAC ACTION and or supported modes HVAC MODE . The user then specifies when Home Assistant reads those two registers, what the values actually mean in the context of a thermostat (the values configuration variables). The active hvac state is read out of the device and interpreted according to the hvac action supported list. The hvac mode can either be read from the device and interpreted according to the hvac mode supported list, or it can be written to the device using the async set hvac mode() function. The only possible thing that I can think to do to make this more generic than it already is would be to allow the user to set the register types (holding register, etc), count (number of bytes), structure (python struct type), etc. It would add more configuration variables, but I\'m willing to do it if that\'s what\'s necessary to get this PR approved.']"
6,319,6_coordinator_coordinatorentity_coordinators_coordinatordata,"['coordinator', 'coordinatorentity', 'coordinators', 'coordinatordata', 'coordinated', 'entity', 'entities', 'selfcoordinatordata', 'selfconfig', 'registered']","['type coordinator', 'Already done by the coordinator entity', 'It should always have a coordinator? How would it be possible to not have the coordinator?']"
7,245,7_state_states_states1_stateobj,"['state', 'states', 'states1', 'stateobj', 'class', 'hassstatesget', 'property', 'status', 'unknown', 'information']","['Does this need to check for STATE UNKNOWN ? Added.', 'If a state is unknown, we need to return None .', 'Why the state?']"
8,242,8_brightness_colormodeunknown_colors_colour,"['brightness', 'colormodeunknown', 'colors', 'colour', 'rgb', 'color', 'rgbw', 'hue', 'saturation', 'light']","[""Using only an RGB tuple to keep track of a light's color brightness usually does not work too well. Does the color handling of the RGB light really work reliably? What happens if you set a color, dim the light to brightness 1, then dim it to maximum brightness, is the light's color preserved or not? From what I remember from testing - yes. I'll go through these scenarios one more time. I'll also look into explaining why it was done this way specifically."", ""Is this correct, or can the LIFXColor and its subclasses support white with adjustable color temperature too? LIFXColor devices all support COLOR MODE COLOR TEMP as well as COLOR MODE HS Thanks Djelibeybi Does it mean this check is not needed, and supported color modes are always hs color temp? Also, it looks like the API sends a tuple (h, s, b, kelvin) to the light. What does the light do if saturation is not None , is the kelvin parameter ignored or still relevant? For color bulbs, they all support both modes. If min kelvin max kelvin then the bulb only supports brightness adjustment. if saturation 0 , the kelvin value is ignored. OK, so would this be correct: This product seems to be a bulb with adjustable color temperature but without color support though: Right, but it wouldn't be discovered as a LIFXColor bulb, though. OK updated so LIFXColor and LIFXStrip report support for color modes hs color temp. If saturation is 0, the light will report the current mode as color temp, else it will report hs."", 'This does not change the behavior compared to without this PR because it would always report a non- None hs color for lights supporting hue saturation which is interpreted as color mode hs by the base entity. However, for lights which support both hs and white mode it would be better to report according to the light\'s state. I did not immediately find anything useful in the library though. Any idea, Jc2k , bdraco? HomeKit (at least as defined in the public spec ( doesn\'t have the concept of an active color mode. You can use h s and you can use color temp. It has the characteristics you can see here, and thats it. There\'s no way to indicate which color mode is currently in use. Sometimes you can figure out a bit more from the open source example code. There are some notes here: L76 Again, theres no characteristic to indicate a color mode there. (via bdraco) says: Does that help? OK, I see. I\'ll add a comment in the code stating that\'s the case. The situation has changed a bit in iOS 13 The spec says that you\'re not supposed to have color and color temperature at the same time, but in practice even certified devices implement that now. Apple seems to be certifying them anyway, and the hope is that the spec will get updated in the next version to remove that restriction. iOS seems to handle it now so it seems like the documentation is behind the actual implementation The way iOS seems to handle it is, whichever characteristic was the last to get a notify event for is the color mode you\'re in For the devices that support both color and color temp you can expect to get a notify for the color which is usually the color temp to color conversion value, and then a notify for the actual color temperature characteristic. Some implementations will not send the notify for the color temperature to color conversion first, and only send the color temperature characteristic notify. In practice that\'s likely not a problem because in the UI we already compute the approximate color of the color temperature. It\'s also consistent with what I posted above which ever notify event that you get last is the color mode you are in The only real caveat is because events are sometimes coalesced if you receive a color and color temperature characteristic in the same event, you should prefer the color temp characteristic for the color mode That\'ll be awkward to implement with the current abstraction. For a start, aiohomekit doesn\'t do any tracking of when the last change occurred. Even if it did, polling and notifications get funnelled into the same state machine and right now we don\'t know which is which. And there is always polling as it was largely consequence free and a good way to recover on networks that have flakey wifi. If conversions are lossy we would potentially see a notification for one colour mode. Then after a minute there would be a poll of all colour values, which might look like a change into a different colour mode. There\'s also the gotcha that we can\'t recover the correct colour mode after a restart. We\'d have to wait for an event. We\'d also need to feed in our own colour mode changes that we generate, as you don\'t get a notify for a change you made through your own connection. At the moment this sounds like a workaround and I\'m not keen on going there unless it\'s a massive problem to not go there. There are only a few that implement both so I think it\'s fine to not support that setup. It does seem to be getting more common though so probably something for the future Hopefully we\'ll get a new spec or an ADK update... (doubt it). It might be less sucky after I\'ve done all the refactoring I want to in aiohomekit (I want the abstraction to be higher level than it is right now). Based on your discussion I don\'t think the simplification I did in 5ee6331 was correct, it should instead be like this: - If the light reports support for both ct and hs, we should expose that as supported color modes {COLOR MODE COLOR TEMP, COLOR MODE HS} (no change compared to before this PR) - Because aiohomekit does not track which color mode the light is in, a light supporting both ct and hs should just report color mode COLOR MODE HS (no change compared to before this PR) Is that right? To be more pedantic on the second bullet: the HAP protocol itself (rather than aiohomekit) doesn\'t really have a concept of a current colour mode to track and aiohomekit\'s abstraction doesn\'t let us work around that. And even if it did, it would be an incomplete work around (we would lose the ""current colour mode"" on a simple HA restart for example). I assume the official iOS implementation is subject to the same limitations too. (The thing I\'m being pedantic about is that despite the changes in iOS 13 it\'s still not the case there is a variable to track, we would have to track multiple variables for multiple event types (push, put and poll) and then infer our own value from the event timestamps of a subset of event types, only to have a partially working implementation at the end of it). In the future we could implement the heuristics described to make it closer to the ideal, but short of Apple adding a new colour mode characteristic there is a potential for our implementation to seem ""glitchy"". So we might not want to implement such heuristics at all. But the ultimate conclusion is the same, I think. OK, updated again. bdraco, Jc2k Is this PR OK now, or are more changes needed?']"
9,238,9_optionsflowwithconfigentry_config_configuration_configflow,"['optionsflowwithconfigentry', 'config', 'configuration', 'configflow', 'configflows', 'settings', 'configtype', 'selfconfig', 'reconfigure', 'configentry']","['There is no options flow', 'Use the config entry', 'get it from the config entry']"
10,219,10_default_defaults_setting_option,"['default', 'defaults', 'setting', 'option', 'enabled', 'normal', 'options', 'disable', 'specify', 'optional']","['That is the default', 'This is the default.', 'This is the default.']"
11,200,11_testing_test_tests_tested,"['testing', 'test', 'tests', 'tested', 'untested', 'check', 'asserts', 'failing', 'fail', 'huumtest']","['Please keep this test.', ""Why can't this test be part of the above one?"", 'Please add a test for this case Tests fixed']"
12,196,12_pr_prs_prt_separate,"['pr', 'prs', 'prt', 'separate', 'merge', 'merged', 'removed', 'lets', 'split', 'change']","['Can you move this to a separate PR please?', 'Separate PR please', 'This should not be here in this PR.']"
13,194,13_constants_constant_declare_numbers,"['constants', 'constant', 'declare', 'numbers', 'pattern', 'standard', 'bound', '7e', 'a20726d', 'strings']","['Use constants:', 'Please use constants for this', 'Please use constants']"
14,178,14_icon_icons_iconsjson_logo,"['icon', 'icons', 'iconsjson', 'logo', 'device', 'entity', 'remove', 'class', 'ui', 'smiley']","['Icon should be removed now it has a device class?', 'This entity has a device class, thus should not set an icon', 'This entity has an device class and should therefore not have an icon']"
15,141,15_removing_remove_removed_unused,"['removing', 'remove', 'removed', 'unused', 'gone', 'cleaned', 'appears', 'needed', 'handy', 'kept']","['Can also be removed', 'Can be removed', 'Not used, should be removed Removed']"
16,132,16_datetimedatetimeisoformat_datetime_sysstrptimetime_utc,"['datetimedatetimeisoformat', 'datetime', 'sysstrptimetime', 'utc', 'timestamps', 'timezone', 'fromtimestamp', 'timestamp', 'timezones', 'objstrftime']","[""I'd create a constant at the module level for a datetime instead. We aren't interested specifically in the start of local day datetime or using the today at template function. We want to test the as datetime template function. Then it will be easier to set a parameter for the expected result. I was struggling with this one. I want to test with a datetime object as input (so not a datetime string). But if create a datetime object, and use it as input, it will use the python representation, so something like datetime.datetime(2024, 1, 1, 0, 0, 0) and then the tests will throw an error that it gets an integer where a comma is expected. That's why I create a datetime object in the template itself. I did update the test. I fixed the date to 1st of January 2024 so the output can be expected. I could also use as datetime instead of today at but then I would be using the function I want to test in the test itself. today at and as datetime are the only options I could think of which give predictable output. now() will change while the test is ongoing. example with a datetime.datetime as input: Changed it to use as datetime for the input"", 'Does this return a timezone aware datetime object from the library? Yes, and in local time', ""Why do we do this? Wouldn't it be better if we saved the expiration as epoch? This way we don't have to trouble ourselves with the timezones, since its all UTC I had to introduce this, as otherwise I had problems with the reading of the config entry on line 41: I assume, in case of the setup or the migration the data is in the cache and without the conversion on line 82 would be available as datetime. Consequently, line 41 throws an error. Once saved in the file, it is read as str and therefore I read it in with fromisoformat . Instead of checking the type in async setup entry I thought it is more efficient to convert it in the config-flow (and now also in the migration). With regards to saving the epoch, I do not yet see how this could be realized, as one would then also have to save the starting date, otherwise we do not know when the epoch ends. Or am I missing anything? You know the epoch is x miliseconds after 01-01-1970? No, I was not aware that you are referring to this. Is there a difference in the result? I guess the datetime would have to be saved as timestamp() and then read with fromtimestamp() , but besides this I don't see an advantage. An advantage could be, if the whole logic would be adapted from current local time to UTC, but this would require a rewrite of the client and integration, and I'm not sure if this is reasonable in the case at hand. The biggest advantage is that epoch is in UTC, so you don't have the whole timezone mess as it just works So instead of saving 2024-12-31T10:00:00 00:00 you're saving 1735603200 . I always find to have a love hate relationship with iso date strings. Yes, it's a little less readable, but it saves some headaches with timezones and people switching timezones probably I agree, that if I would start from scratch, UTC might be easier. But if you agree, I would prefer to keep it as is (and perhaps adapt the logic in a separate PR at a later stage). The only point I don't get is why you say it would affect the library? (If I understood correctly). Also keep in mind, if all users now have an iso string in their entry data, a migration is required to migrate that to an epoch, so it would be benificial if we can avoid that migrationcode If I look at the changes, we could just replace the way to serialize the timestamps. Unless the expiration isn't always of type datetime, but then I am interested in why this type can be a different type. My point was, that at the moment the whole code is with localized datetime. The benefit of saving a UTC time (epoch or datetime-string) only applies, if the whole code would be converted from localized datetime to UTC (which would require an adoption of the library as well). As long as we have localized datetime in the middle, we have to handle conversion. I think you are right. But in my view this goes beyond the scope of this PR (which according to my understanding should be as narrow as possible), as said above, it would also need an adjustment of the library. To make my question a bit more explicit: Where does the timezone affect the library? I think I am missing that point here. If I understand the code we get a datetime object back, and we do the check if the token is still valid on the integration side, so I don't see where the lib should change. (Oh and I am with the intentions of getting this in, I'm just trying to do my due dilligence) Oh wait, you are passing the tz and expiration to the lib. I am wondering why, since we always refresh in the coordinator. The datetime returned from the library is local time ( datetime.now() ). For consistency purposes this should be amended to UTC, otherwise we still have to convert. Right I think I see the problem. Let's do it this way. But now the next thing I was wondering, why do we have the isinstance(credentials[CONF EXPIRATION], datetime) . Can it ever be not a datetime object? I think you are right. This would not be needed to be passed on. You got a point, at this point it should always be a datetime. Can we do without this check or does mypy complain then? In that case I would suggest using a cast mypy seems to be fine with the omission.""]"
17,130,17_yaml_configurationyaml_config_yarl,"['yaml', 'configurationyaml', 'config', 'yarl', 'configuration', 'import', 'importing', 'servicesyaml', 'imported', 'serviceyaml']","[""When converting to config flow this is usually replaced by an import of the configuration.yaml data into a config entry. Do you have an example of this? I don't really understand what you are meaning here. Do I have to create a config entry when loading the configuration.yaml? Yes, that's exactly right. You'll start the config flow at a special import step that will create a config entry based on the data from configuration.yaml. [This]( is an example in one of my open PRs of importing a configuration.yaml entry. See the setup function. It is for an integration configured at the top level of the configuration.yaml. Yours is configured as a platform so the details will be slightly different."", 'Can be removed as it is only used by the yaml configuration', ""There's no config YAML for this integration.""]"
18,124,18_intervals_polling_interval_intervall,"['intervals', 'polling', 'interval', 'intervall', 'scan', 'schedule', 'servicehomeassistantupdate', 'automation', 'updates', 'updating']","[""We don't allow scan interval as option. Every integration that supports config entries allows the user to turn off automatic polling and then the user can automate the update interval as needed with the entity service homeassistant.update entity ."", ""We don't allow scan interval as option. Every integration that supports config entries allows the user to turn off automatic polling and then the user can automate the update interval as needed with the entity service homeassistant.update entity ."", 'Why should users be able to change the polling interval? Is here no polling interval that you can set by default, that will work for every user?']"
19,123,19_simplified_simplify_simpler_reduced,"['simplified', 'simplify', 'simpler', 'reduced', 'clearer', 'easier', 'unclear', 'harder', 'shorter', 'vague']","['Can this be simplified?', 'It can be simplified.', 'Maybe a bit more descriptive?']"
20,119,20_assistant_homewizard_homeassistantreworktahoma_homeassistantcore,"['assistant', 'homewizard', 'homeassistantreworktahoma', 'homeassistantcore', 'homeassistant', 'webhooks', 'api', 'agent', 'home', 'automations']","['Is ""Home Assistant"" the USER APP NAME ? I\'d use that constant in that case.', 'We should get the default language from Home Assistant.', 'home-assistant core']"
21,117,21_alphabetically_alphabetical_alphabetized_alphabetize,"['alphabetically', 'alphabetical', 'alphabetized', 'alphabetize', 'alphabetic', 'alphabet', 'sorting', 'order', 'sort', 'ordered']","['Alphabetical order :)', 'Usually we sort this alphabetical', 'Could you sort in alphabetical order?']"
22,116,22_hassdatadomainconfig_hassdatadata_hassdatadomain_hassdatasetdefaultdomain,"['hassdatadomainconfig', 'hassdatadata', 'hassdatadomain', 'hassdatasetdefaultdomain', 'hassdatadomainentryentry', 'hassdatadomainentry', 'hass', 'hassdata', 'entrydataconf', 'storing']","['Only store in hass.data once you are sure the integration is going to set up', 'Please store data in hass.data[DOMAIN][entry.entry id] as a dataclass', ""It looks like a lot of the certificate logic could be moved into the hassmpris client library. We want Home Assistant to know as little as possible about the integration implementation details The thing is, this code (casting certs and keys from bytes to objects) is only needed because HASS insists on storing strings bytes -- the integration only deals in actual typed Python objects, like Certificate and Key . Short of moving these casting functions into helpers within the client, this actually belongs here and not in the MPRIS client. All of the logic of actually obtaining the certificate and pairing with the remote end is fully sealed up in the MPRIS client. What should I do? I'm 100 new to writing HASS code, I need your help. Thanks in advance. While we want to store configuration data in the config entry, certificates are a bit large to put in there since everything has to be serialized to JSON and back. Config entry storage isn't a database so every time its loaded saved all the data for all the config entries has to be processed. You can storage large data in .storage You can accomplish this in two different ways. You can pass the storage path to the library with hass.config.path . git grep for a few examples in the code base, and let it be responsible for reading the certificates from disk (in the executor please!) Also you can use homeassistant.helpers.storage to store the certificates. There is an example in homekit for aid storage. I almost always prefer the solution that has the least code in HA itself and offloads that to the PyPI package code since it reduces the review time to make changes to an integration and keeps things moving along. Roger that. The client module has no provision to store anything on disk -- it is stateless -- so I will factor out the certificate load save code to their own separate functions and proceed as you advised with regards to mirroring what homekit does.""]"
23,115,23_subclass_subclasses_class_classes,"['subclass', 'subclasses', 'class', 'classes', 'inheritance', 'classvar', 'inherit', 'base', 'constructor', 'superclass']","['base class', 'This should already be set by the parent class', 'Set by the parent class']"
24,114,24_block_tryblock_trydanerror_blocks,"['block', 'tryblock', 'trydanerror', 'blocks', 'wrap', 'fail', 'line', 'exception', 'exceptions', 'trycatch']","['Please only wrap the line that can raise in the try... except block. Use an else: block if needed.', 'Please only wrap the line that can raise in the try... except block. Use an else: block if needed. Done', 'Please only wrap the line that can raise in the try... except block. Use an else: block if needed.']"
25,113,25_deviceinfo_deviceclassrestart_deviceparent_devicecover,"['deviceinfo', 'deviceclassrestart', 'deviceparent', 'devicecover', 'devices', 'device', 'firmware', 'ios', 'selfdeviceapivapix', 'objectobject']","['Please return a DeviceInfo instead', 'Please return a DeviceInfo', 'Just set self. attr device info in your constructor. And use the DeviceInfo tuple from homeassistant.helpers.entity import DeviceInfo .']"
26,111,26_playback_media_playerpy_spotify,"['playback', 'media', 'playerpy', 'spotify', 'airplay', 'play', 'audio', 'music', 'radio', 'roonradio']","[""This needs to be used in the media player play media command too? When browsing with the Spotify media player itself, there's no config entry id added, it's only needed for users of spotify.async browse media . In the current codebase, only sonos needs to be updated."", 'Why does this need a new service? This sounds like something you can just pass to the play media service. I believe that the HA Media Player doesn\'t support the Roon concept of \'Radio\', thus may not be able to be neatly passed using the play media service. If you know of a neater way for this however, it would be greatly appreciated. The intent is to be able to start a Roon radio from a given artist or genre which is currently not possible. The RoonAPI Play Media function is: play media(self, zone or output id, path, action None, report error True) Where action can be: Null (for the default play action), or ""Play Now"", ""Queue"" or ""Start Radio"". The code here for its play media doesn\'t pass the \'action\' parameter to the roonApi play media function. In addition, there is no way to request the \'Start Radio\' action through the \'path\'. A separate service for \'Start Radio\' would allow the usage of this Roon concept. balloob is there a way to pass an extra parameter to play media ? There was a previous proposal to reuse the media type parameter for this which I didn t think you guys would be comfortable with. The Home Assistant Play Media API can process any incoming data. So you could have a URL format like roon-radio: chill-mix?artist Donnie . You can then expose these items via the media browser so users don\'t actually have to craft these URLs themselves. So given balloob\'s comments suggest you go back to your previous PR - and we find a way to delimit the action at the start or end of the current path string. One option would be to enhance the pyroon split path function to do this. Starting music in ""radio"" mode is pretty common and used by other (potential) HA integrations as well (e.g. spotify). A separate service is a bad idea imo bit passing an additional argument to the play media service call is great. It could even potentially be a new Enqueue Option ? My proposal would be if you could quickly draft an architecture discussion about adding this to the play media service as an additional parameter or enqueue option. Yes, the suggestion with constructing the custom uri is also possible but that would be a bit cumbersome for users to figure out in automations. Hi, I ve pulled together notes for an architecture discussion as a starting point, attempting to summarise the options suggested thus far and potential pros cons with each. Appreciate the support and assistance towards implementing this functionality that will be a really powerful enhancement allowing the selection of music to play through HA automations. Let me know if there s somewhere better to place this content for appropriate collaboration. The purpose for this architecture discussion is to collaboratively identify the best method to enhance the HA play media service to support requesting music to start in a radio mode. This mode is implemented differently by various music providers, however generally it is a way of starting music playing of a given Artist Song Genre and then the player continues to select tracks similar to the given source (rather than loading a list of tracks into the media player queue). I would personally use this to implement automations to play certain genre mixes of music at various times of the day, or even to allow playing of a random genre or artist mix with the press of a button. The HA media player currently supports two parameters related to starting music playing: path and a media type . Music providers may support the method of starting a radio mode in various implementation-specific ways. For example, the Roon music service supports this in their API call for play media by passing not only a path parameter, but also an action parameter. This action parameter specifies the play action and can either be passed as: Null (which uses a default play action); Play Now ; Queue or Start Radio . Using this parameter is the only way supported by the RoonAPI to start a Radio mode (the Roon API does support specifying a play action through the path parameter, however this is limited to a play action such as play artist or play genre but does not support specifying the Start Radio action). The HA Roon Integration currently passes the HA media player path parameter to the RoonAPI play media function without specifying the action parameter. Options to implement this within HA could be: Option 1: Reuse the HA media type parameter to pass to the Roon API as the action parameter in the Roon API play media function call. Thus a user could specify Start Radio as the media type , and the HA Roon integration media player code would be changed to pass this value to the Roon API call as the action parameter. Advantages of this are that it is using an existing parameter and is simple to implement. Disadvantages are that it only implements this for Roon media players (and not other music providers), and that it is a bit of a hack as the media type parameter is not intended to be used as a play action. In addition, it would not be obvious to users how to utilise this feature. Option 2: Delimit the action at the start or end of the path parameter, potentially enhancing the pyroon split path function to do this. Advantages of this are that it is using an existing parameter and is simple to implement. Disadvantages are that it would not be obvious to users how to utilise this feature. Option 3: Create a new HA service for the HA Roon Integration to perform a request to start a Roon radio . Advantages are that it creates a distinct service for this Roon specific functionality obvious to users and is relatively simple to implement. Disadvantages are that it only implements this for Roon media players (and not other music providers), thus services would need to be created uniquely for each music provider s HA Integration. Option 4: Use a URL format like roon-radio: that the AH Play Media API can process to make an appropriate call to the Roon API. Items could be exposed via the media browser so users don t actually have to craft the URLs themselves. I am not familiar with this approach, so would appreciate further collaboration on the pros cons of this option. Option 5: Add an additional argument to the HA Play Media service to directly support the play action request (aligning with the way the RoonAPI uses a specific parameter for this). Advantages are that it is obvious to users and would align the implementation of this capability across all media player implementations. Disadvantages are that it may not be supported by all music providers and may be more complex to implement as it impacts all media player integrations (and not just the HA Roon Integration). I\'d change the text a bit as this is a generic proposal for all media players, not specific to Roon. Option 6: Add Radio to the Enqueue enum, which is an existing option to the play media call. thanks, I\'ve made more generic and reordered the options (btw, I\'d appreciate advice on how where to raise this once were happy with the content): The purpose for this architecture discussion is to collaboratively identify the best method to enhance the HA media player\'s play media service to support requesting music to start in a radio mode. This mode is implemented differently by various music player providers, however generally it is a way of starting to play music of a given Artist Genre and then the player continuing to select tracks similar to the given source (rather than initially loading a list of tracks into the media player queue). The HA media player\'s play media service currently supports the following parameters related to starting music playing: \'media content id\' - The path ID of the content to play. Platform dependent. media content type - They type of content to play. \'enqueue\' - If the content should be played now or be added to the queue (Play now Play next Add to queue Play now and clear queue). \'announce\' - If the media should be played as an announcement (true false). Music providers may support the method of starting a radio mode in various implementation-specific ways. For example, the Roon music service supports this in their API call for play media by passing an action parameter to initiate this mode. This action parameter specifies the play action and can either be passed as: Null (which uses a default play action); Play Now ; Queue or Start Radio . Options to implement this within HA could be: Option 1: Add \'Play Radio\' to the \'Enqueue\' enumeration field as an option. Advantages are that this is an existing field in the play media call. Option 2: Add an additional field to the HA Play Media service to directly request to \'start radio\'. Option 3: Delimit the action at the start or end of the \'media content id\' \'path\' parameter, potentially enhancing the pyroon split path function to do this. Advantages of this are that it is using an existing parameter. Disadvantages are that it would not be as obvious to users how to utilise this feature. Option 4: Create a new HA service for each provider\'s media player integration to perform a request to start a \'radio . Advantages are that it creates a distinct service for this specific functionality. Disadvantages are that individual services would need to be created uniquely for each music provider s HA Integration and therfore not be standardised across HA media players. Option 5: Use a URL format like roon-radio: that the AH Play Media API can process to make an appropriate call to the Roon API. Items could be exposed via the media browser so users don t actually have to craft the URLs themselves. Thanks again for the support!', 'Move this and the other constants which are used by the media player platform to media player.py']"
27,109,27_attrs_attrsasdict_attr_attribute,"['attrs', 'attrsasdict', 'attr', 'attribute', 'attributestarts', 'attributes', 'icon', 'init', 'use', 'code']","['Use attr name', 'also use the attr', 'Could be attr...']"
28,104,28_schemadata_schema_schemareauth_schemas,"['schemadata', 'schema', 'schemareauth', 'schemas', 'defaults', 'default', 'foreignkey', 'usedoncemethod', 'ondelete', 'optional']","[""We can remove the config schema. It's not used."", 'Move the default into the schema instead.', 'I think that you could make the schema a constant (with default 1), and then use: data schema self.add suggested values to schema(DATA SCHEMA, self. current)']"
29,102,29_measurement_units_meters_measurements,"['measurement', 'units', 'meters', 'measurements', 'unit', 'meter', 'measure', 'millimeters', 'unitoftime', 'imperial']","['Please dont have the unit of measurement in the name', 'Unit of measurement should not be in here', 'Unit of measurement?']"
30,97,30_formatting_formatter_format_fstring,"['formatting', 'formatter', 'format', 'fstring', 'fstrings', 'string', 'stringify', 'formats', 'strings', 'autoformatting']","['Use string formatting using f-strings.', 'string formatting.', 'No need to put in string formatting.']"
31,95,31_debug_loggerdebug_debugging_debugspecific,"['debug', 'loggerdebug', 'debugging', 'debugspecific', 'logging', 'logs', 'log', 'logged', 'logger', 'troubleshoot']","['Do we really need to info log this? I think debug is fine.', 'Please remove this one, we should not log on debug', ""I'd not log this above debug level.""]"
32,94,32_fixture_fixtures_fixtureforecastjson_bulbs,"['fixture', 'fixtures', 'fixtureforecastjson', 'bulbs', 'bulbscanner', 'mock', 'testing', 'tests', 'test', 'testflow']","['Should this be a test fixture instead?', 'Use the fixture', 'move this into a fixture']"
33,94,33_uniqueidrequirements_entryid_unique_identifier,"['uniqueidrequirements', 'entryid', 'unique', 'identifier', 'ids', 'uuid', 'byid', 'id', 'ulid', 'iitemid']","['Why would we not want a unique id ?', 'You can also use entry.entry id as a unique ID.', 'Not a good unique id. Rather use entry id .']"
34,94,34_eventevent_event_events_calendarevent,"['eventevent', 'event', 'events', 'calendarevent', 'calendar', 'calendars', 'notifications', 'notification', 'trigger', 'data']","['can you also check the data of each event.', 'we use event type', 'Is the event name included in the error? no, this happens when there is an issue fetching all new events, before single events are constructed. changed ""new event"" - ""new events""']"
35,91,35_entityunifientityasync_async_entity_entities,"['entityunifientityasync', 'async', 'entity', 'entities', 'entityget', 'entityargument', 'callback', 'entitiesgsmsignalsensorhass', 'instantiates', 'addition']","['Please only call async add entities once', 'async add entities should be call only once.', 'Can we do this in one async add entities call?']"
36,91,36_typed_typing_typings_type,"['typed', 'typing', 'typings', 'type', 'complete', 'completed', 'uncomplete', 'typingcallable', 'enter', 'write']","['typing? Done', 'Typing Done', 'typing done']"
37,90,37_necessary_shouldnt_required_needed,"['necessary', 'shouldnt', 'required', 'needed', 'use', 'need', 'extra', 'reminder', 'question', 'modify']","['Why is this needed?', 'should not be needed', 'This should not be needed?']"
38,90,38_import_importing_imports_imported,"['import', 'importing', 'imports', 'imported', 'exported', 'unpack', 'package', 'unpacking', 'files', 'need']","['There is nothing to import.', 'and please import those', 'Please import this one']"
39,89,39_thanks_awesome_thx_nice,"['thanks', 'awesome', 'thx', 'nice', 'gotcha', 'great', 'nicer', 'hint', 'ah', 'forgot']","['Good call. Thanks', 'Good catch thanks', 'Ah, nice. Thanks.']"
40,88,40_exceptions_exception_catch_catching,"['exceptions', 'exception', 'catch', 'catching', 'avoid', 'caught', 'allow', 'fail', 'smtplibsmtpresponseexception', 'roborockexception']","['Can we catch more specific exceptions? We only allow the config flow to catch bare Exceptions Modified', ""You're not allowed to catch broad exceptions here"", 'Are there no specific exceptions in the module that you can catch? Broad exceptions like this are not really nice.']"
41,88,41_integrationspecific_integrationdimmer_integrationswitch_integration,"['integrationspecific', 'integrationdimmer', 'integrationswitch', 'integration', 'integrations', 'integral', 'resolved', 'entry', 'create', 'adapted']","['We normally patch the integration setup function on all create entry results to avoid setting up the whole integration.', 'Please patch the integration setup function on all create entry results to avoid setting up the whole integration.', 'Please patch the integration setup function on all create entry results to avoid setting up the whole integration.']"
42,85,42_enums_enum_enumname_enumstrenum,"['enums', 'enum', 'enumname', 'enumstrenum', 'enumerations', 'enumerator', 'intenum', 'enumeration', 'homeassistantbackportsenum', 'ordinal']","['Could you do a PR to to add this as an enum? Done:', 'Please use the enum value here', 'Should this be an enum?']"
43,84,43_requiredkeysmixin_keys_keys1_remove,"['requiredkeysmixin', 'keys', 'keys1', 'remove', 'key', 'blank', 'keyboard', 'lenkeys', 'unused', 'keystroke']","['please remove any empty keys', 'Remove empty keys', 'Remove empty keys Done.']"
44,81,44_titles_title_renaming_capitalization,"['titles', 'title', 'renaming', 'capitalization', 'manifest', 'manifestjson', 'translated', 'branddomain', 'description', 'logo']","['Remove the title, just keep it the default.', 'The name from the manifest is automatically used as title, only set the title in strings.json if it should be translated or if it is different from the manifest.', 'Do we need a title?']"
45,79,45_revert_reverted_reverting_changed,"['revert', 'reverted', 'reverting', 'changed', 'change', 'fixed', 'fix', 'fixing', 'changes', 'bug']","['Revert this change', 'Please revert this change.', 'Please revert this change.']"
46,78,46_completed_thanks_thank_doable,"['completed', 'thanks', 'thank', 'doable', 'thx', 'check', 'want', 'sorry', 'cached', 'comment']","['Thanks, done!', 'Thanks! Done', 'Thanks! Done']"
47,78,47_file_filenotfounderror_remove_deleting,"['file', 'filenotfounderror', 'remove', 'deleting', 'files', 'delete', 'removed', 'directory', 'script', 'unsupported']","['Remove this file.', 'move it to its own file', 'This file should not be here. Please remove this file. Done']"
48,78,48_executor_execution_executed_trystatement,"['executor', 'execution', 'executed', 'trystatement', 'functoolspartial', 'selfhassasync', 'async', 'queue', 'asyncioevent', 'await']","['This needs to be run in the executor', 'If you use the async method then there is no need for the executor job.', 'why the executor job?']"
49,75,49_deviceserial_entity_entities_deviceinfo,"['deviceserial', 'entity', 'entities', 'deviceinfo', 'devicedevice', 'entitynaming', 'devices', 'device', 'naming', 'productname']","['device info can be set in the constructor with attr device info . Then you can set attr name None and the entity will follow the device name', 'Could we calculate the prefix by only using the device point.category ? Eg by looking for a separator in the string or taking a certain number of characters from the beginning etc? I\'d like to avoid having a tuple of known prefixes to match against and needing to maintain that tuple. This is a difficult one... There is no simple way to categorize the models. The product,name appears on different levels in the object hierarchy and the device point.category is not an exact match to the product.name. My interpretation of the data model in the API: 1. An account can containg many systems. 2. A system is a heat-pump appliance. 3. A system can contain one or more devices 4. A device contains many device-point. 5. There is no official documentation from the manufacturer on the actual data that each model reports. Example from my F730: One system with one device with some 45 device points. product.name in system object: ""F730 CU 3x400V"" a single device with product.name: ""F730 CU 3x400V"" category in all device points: ""NIBEF F730 CU 3x400V"" Example from pajzo: product.name: unknown to me category in device points: Mostly ""NIBE VVM S320 E EM 3x400V DK"" but mixed with ""Varmepumpe 1"" I am out of ideas (for a short moment - I hope). I am a bit sorry that the search for a perfect solution is blocking other areas of improvements. It is possible to create generic binary sensors, generic switches and other suitable entities. It is when it comes to fine-tuning device classes, icons, display precision etc that we are stuck. The current solution will work for Fxxx -series appliances. All other models will show up with generic sensors as in the original release. I suggest that we merge this version. We can then revisit the challenge of classifying appliance models when we have collected more diagnostic data. I agree with you astrandb, i think it\'s fine to map based on parameter id, and fallback on paramter unit. Then we over time can improve by adding more and more parameter id mappings. We now have confirmation thay S series have a different numbering scheme which may overlap. So we must discern them. We also now see that S series prefix by ""NIBE"" and F series by ""NIBEF"". That is enough. Just use partition on the category and grab first words. Use a double level dict. For lookup. Also avoid putting unit lookups and paramater id lookups in same dict. Thank you elupus for you constructive ideas. They are implemented now.', 'Since the device you set as device info has the same name as this entity. This will name the entity to the device.']"
50,75,50_mock_mockconfigentry_mocks_mocklifxcommand,"['mock', 'mockconfigentry', 'mocks', 'mocklifxcommand', 'mocksetup', 'mocked', 'mocking', 'mockurlimageentity', 'mockhaclientwebsocket', 'unittestmock']","['No need for this. You can just check the mock entry . It will use the same object', 'It would be good to type the mock as well', 'we need mock setup entry in the test']"
51,75,51_pytestmarkparametrize_pytest_pytestmark_pytestraises,"['pytestmarkparametrize', 'pytest', 'pytestmark', 'pytestraises', 'tests', 'unittest', 'test', 'mock', 'assert', 'tested']","[""We don't write tests like these anymore. Write tests like done for the config flow. For this one, what I understood is that the test should use the pytest format and not the older unittest like classes? correct. We only write function-based tests. Use pytest.fixtures if you want to run code at startup shutdown of tests."", 'Instead of running tests like this, you can use pytest.mark.parametrize and then you can create 1 test and have the data as a parameter.', 'The two tests test switch on and test switch off can also use pytest.mark.parametrize as below.']"
52,73,52_variable_variables_underscore_underscores,"['variable', 'variables', 'underscore', 'underscores', 'var', 'assign', 'code', 'lowercase', 'assigns', 'usecase']","['Where are the code to use this new Variable ?', 'Please separate words with underscore in parameter and variable names. friendly name', 'Please separate words with underscore in variable names etc.']"
53,73,53_type_types_hints_hinting,"['type', 'types', 'hints', 'hinting', 'typechecker', 'typing', 'supertype', 'addentitiescallback', 'generics', 'generic']","['Please add type hints', 'Add type hints', 'Please add type hints Done']"
54,72,54_entityservices_hassdatadomainservices_services_entityserviceresponse,"['entityservices', 'hassdatadomainservices', 'services', 'entityserviceresponse', 'service', 'serviceintenthandler', 'register', 'registering', 'registration', 'entity']","[""Shouldn't we keep this within the domain of this integration? hass.data[DOMAIN][SERVICES] Do we really need to check again registering this double? Like would it matter? This is done like in the deConz integration. Both are quite similar in regards to supporting multiple instances and having services. Right now the integrations reside directly under hass data unifi. I would need to add another layer for every place retrieving an instance of the integration if we want to move services to hass data unifi services. If there is no control the first integration to be removed would remove the services as well? Maybe I don't understand your second question? The unload of services is not done by UNIFI SERVICES , it uses: The point is, that UNIFI SERVICES and its deduplication isn't needed? Aah ok, I should verify UNIFI DOMAIN being empty during async setup entry and remove anything with UNIFI SERVICES instead right? That would have the same effect. Additionally, I don't think (not 100 sure, MartinHjelmare can probably confirm), registering a service twice isn't a problem doesn't have side effects. So I think we don't need the logic that prevents them from registering twice. It will send out multiple events L1351 and that's about it, I'm just trying to keep stuff tidy :) Hmmm right :) I guess we send out a duplicate event quite a bit in general (in case of multiple config entries). I wonder if that is something we should solve on a different level Anyways, it's fine I think the UNIFI SERVICES can be refactored out while still achieving the same and some docs and it should be good to go. Can we merge this as is and I will do a follow up PR to clean this up tonight? Would be nice to not miss the beta Added documentation PR I think that is a good deal If we want to avoid doing the work of registering the service again we can check if the service is registered already with hass.services.has service . Resolved in 56834"", 'Please use our entity service helper to register the services instead. entity-services', 'Only register the services if no config entry has registered them yet. We have hass.services.has service .']"
55,72,55_commentedout_remove_code_commented,"['commentedout', 'remove', 'code', 'commented', 'removes', 'removed', 'tweak', 'omitted', 'comment', 'comments']","['Please remove commented code', 'Please remove code that is commented out.', 'Please remove any commented out code Done']"
56,72,56_platforms_platform_platformnotready_platformpropertiestobeimplementedbyderivingplatformclasses,"['platforms', 'platform', 'platformnotready', 'platformpropertiestobeimplementedbyderivingplatformclasses', 'pr', 'os', 'prs', 'rpi', 'rpi2', 'rpi0']","['This PR has multiple platforms, please limit it to a single platform', 'This PR has multiple platforms, please limit it to a single platform.', 'This PR still has multiple platforms. Please limit the PR to a single platform.']"
57,69,57_library_patchobject_patch_patched,"['library', 'patchobject', 'patch', 'patched', 'patches', 'lib', 'patchtessie', 'fixed', 'package', 'instead']","['Patch the library instead of our code in the config flow.', 'We should patch the library and not our own code.', 'Patch the library instead.']"
58,69,58_notifyentity_notifications_notification_notify,"['notifyentity', 'notifications', 'notification', 'notify', 'platformnotify', 'service', 'message', 'services', 'warning', 'subscription']","[""We don't allow overriding a builtin service. That would be confusing to users. If there's no message to send the integration shouldn't implement a notify platform. The other PR that implements a custom service is ok. But the service is not overriden Instead an additional service with a custom service schema is registered alongside the builtin service from the notify platform. This is currently required, as the notify entity service does not allow extra data keys like the legacy notify platform (required for item field). I also changed notification type to message, as that is basically the message that is sent, just that only 4 predefined keywords are allowed. In contrast to other notification services, bring does not have a free text input for the message. The custom service schema does therefore define the message field as radio input. I know the new notification entity component is still a work in progress, joostlek asked me if i could implement this feature with it. Maybe this can serve as some kind of use case for the further development jbouwh"", ""Can be removed Ok, so let me try to explain what's going on. I am not an expert of the hass internals at all and unfortunately the documentation seems kind of poor when it comes to describing how all the config setup works internally. Here's what I understand: There's an old and a new way of setting up components and entries. The modern way to set up integrations for platforms is to forward the entry setup to the corresponding platforms like this: However, the notify integration seems to be the only one that is stuck with the legacy way of setting up things. This means that the above call will fail with AttributeError: module 'homeassistant.components.notify' has no attribute 'async setup entry' for notify. This is the reason why for example the slack integration separates the behavior for notify like this: L72 Note the if platform ! Platform.NOTIFY and the async load platform call above. This seems to be a common way to setup notify platforms, here they even have an explanation: L137 no entry support for notify platform yet, have to use discovery to load platform. Seems everyone else does the same. You can also check the discord integration or others. This is why I use discovery.async load platform . Now let's come back to how this is related to keeping def async setup : In the above method which you suggested to remove, I store the hass config in hass.data[DATA HASS CONFIG] . Later on, this variable is needed in the call to discovery.async load platform , see the requirement here: L154 Hence, it seems I cannot remove async setup . Let me know if I'm missing something here. Remember when I said that a notify integration to config flow isn't that different? I take that back. This is awful (not because of you btw, you're putting in great effort, I just don't like how it differs from the rest) Like I've seen some pass by but I did not know all these pieces were needed. So I guess I'm also still learning here. Can we mark this resolved? I'm on mobile right now, I'll drive into this tomorrow"", 'I dont think its nice to notify every entry, why not a service per notify']"
59,68,59_l1314l1324_l2030l2041_l135l158_l371,"['l1314l1324', 'l2030l2041', 'l135l158', 'l371', 'l1376l1380', 'l37l39', 'l372', 'l146', 'l463l472', 'l149l155']","['We have a helper now to do this: L2030-L2041', 'Can we use this helper? L1376-L1380 Done.', 'We can use this helper: L1314-L1324']"
60,68,60_logger_logginggetlogger_logging_loggers,"['logger', 'logginggetlogger', 'logging', 'loggers', 'log', 'logs', 'caplog', 'caplogfixture', 'debug', 'pipdeptree']","['This logger does not exist', 'Why are you passing in a logger if you made one on line 24? If you like, you can also create an integration level logger with LOGGER logging.getLogger( package ) in const.py', 'Where is the ""logger"" ?']"
61,66,61_websocket_websocketpy_websockets_api,"['websocket', 'websocketpy', 'websockets', 'api', 'async', 'client', 'connection', 'callback', 'notification', 'signals']","['This is just testing the websocket api. Right. The function itself is tested in the library.', 'We should put this test in test websocket api.py', ""The library client should use this auth instance. It looks unused now. We just instantiate it but then don't use it. When the library client wants to get an access token to use, ie before every request, it should use async get access token defined in this class. The library doesn't seem to have been updated to allow this. I recommend reading our library guide on how to structure the library and client: I've reworked the library according to the documentation. I actually like this approach. Everything works fine now except of the websocket connection. It's not possible to establish it and I don't get an error. I've tried very long, but wasn't able to get it working. Could you have a look at the library, please? L371 I don't think you should make a loop when connecting the websocket. Just have a connect method that connects the websocket and stores a reference to it on the client. L148-L160 Then have another method listen that fetches incoming messages and iterates over them and handles them. It's this method that the library user should create a background task for. L374 The benefit of separating the connect and listen methods is that we can make sure we can connect the websocket with a timeout before we create the task that should listen for websocket messages. If we can connect the websocket the chances are high that the websocket communication will work. If we fail to connect during the timeout, we want to raise ConfigEntryNotReady and try again later automatically. Side notes: 1. It looks like there's a double call to update data callbacks: L380-381 L342 2. I'd remove the schedule immediately parameter. The library user can call the callback themselves if needed. L74 If we create a connect method in the library for the websocket, there should also be a disconnect method for the websocket. There is now a close method""]"
62,66,62_locking_lock_locks_unlocking,"['locking', 'lock', 'locks', 'unlocking', 'locked', 'unlock', 'latch', 'unlocked', 'secure', 'door']","['You could simplify the adding of entities with a comprehension: There will be another lock sensor in the near future. So maybe it gets too complicated to read with different classes? What do you mean by ""lock sensor?"" If there are going to be different lock entity classes used, this is still valuable in that case, you should have something else (a dict, a method, etc.) to look up the correct class based on incoming data. Can you please explain, what i have done there? The sensor is called HmIP-DLS and is just a sensor on the state of the lock. It has lockStates, but no motor to change them. I am not yet sure in which device class platform i gonna put that device. I think putting it in lock platform could be the wrong place. Maybe sensor platform. You can\'t alter the lock state? You\'ve implemented those methods: L69-L75 What does, say, await self. device.set lock state(LockState.LOCKED) actually do? You asked me, what other lock entity class i gonna implement. I told you. The lock device, which this PR is about, could open the door. So everything okay.', 'Why is this a template instead of a fixed string? Can you give some example of where it\'s useful to dynamically change the code format? This seemed to be the most straightforward way to support ""require no code at all to lock, but a certain code to unlock"". Or a well-known short code to lock, but a long (possibly dynamic) one to unlock. From a system design perspective, if so desired it is always possible to go to a ""secure"" state (locked) and you need knowledge to go to an ""insecure"" (unlocked) state. Or have it the other way around if the meaning of ""secure"" and ""insecure"" is swapped in a user\'s context. This works neatly with the frontend without introducing new attributes like [Template Alarm\'s code arm required]( code arm required). OK I see. That makes the PR work around limitations in the lock entity IMHO, something which is not wanted. Please replace the code format template with a fixed code format and hit the ""Ready for review""-button when done. Naturally, I beg to differ I think this makes template locks quite versatile in their application. What is the actual issue here? There are other examples where the current code format for a lock is computed on the fly inside the method, for instance in the implementation of matter locks: L43-L60 Here the return value, at least without forward tracing the code, could change during operation, which is exactly what can happen with a template for lock code formats as well. My conclusion is that there is no actual limitation in the lock entity and thus code formats can be dynamic without any issues. Can you elaborate? (Marking this PR as Ready for review again in order to make this comment visible) Right, the matter implementation computes the code format, but it seems clear from the code the intention is to do so based on properties of the remote lock device, not based on the state of the lock. Is the main driver of your PR to implement support for dynamic code formats? The long story is that I basically wanted to a) virtually secure certain switches complex setups and b) have something similar to a switch that properly supports in-between-states. Meaning something complex is turned on (which I don\'t want to happen by accident and not necessarily by anyone walking through our hallway), and while this is turning on, which can take quite some time, the UI controls display an intermediate state (like the lock that is ""unlocking"" and then is ""unlocked""). The opposite is not true, so while I whish the turning on to be secured, turning off can happen anytime without securing it. That drove me to template locks, which currently don\'t support codes at all, and to make this available and ""templateable."" As far as I see it, the main question is: does any existing code rely on the (undocumented, AFAIK) fact that a code format property of a lock never changes? I guess the answer is no, because there are places where this can obviously occur, Matter being one example. If no one relies on constant code format, why not make it optionally dynamic for template locks (that target advanced users only) as well? And the format can be based on anything, really, not only locks\' state. Another application I have in mind is a dynamic automatic parental control where one can use something while parents are at home, but need to enter a code if they are not. (All this applies to control surfaces that are accessible to anyone in the house). OK, I see, your use case is essentially about (ab)using lock entities for things which are not actually locks. I think what you want to do is covered here although there\'s no solution in that discussion. My personal opinion is that we should allow users to set a flag on any entity marking it as requiring confirmation, optionally with a PIN. Frontend and voice assistants should be aware of that flag and show a confirmation dialog verbally ask for confirmation. For this PR, I would suggest to remove the templating to get the base functionality - adding code format support to template locks - merged. A follow-up PR could extend the code format with templates, and we can continue this discussion in that PR. It\'s not more abuse than [using a light to control home theater volume]( theater-volume-control) . I always fancy the template-integration to be something special that exactly enables flexibility like this. It\'s kind of in its DNA. I am also on board with adding UI-security measures (UI also meaning interactions by voice assistants) to all interactable entities, also with custom messages as described in the linked frontend issue. However, there are three considerations with this idea and your proposed next steps for this PR (all IMO of course): 1. This property (pin required not required) needs to be templateable; user (or role) specific settings would be nice as I wouldn\'t bother myself to enter a PIN to enable A C, for instance, because I usually sometimes kind of tend to know what I do 2. The place where the check happens should be customizable. This means that I expect the actual check (after the requirement of a PIN Code has been determined) to be offloaded to something else to be able to also support changing rolling codes. Only supporting a static setting here would again severely limit flexibility. 3. Introducing a code format for template locks now would solve about half of my issues, but if later we come to the conclusion that it indeed should be templatable, it would either not fit to the template entities config convention ( template suffix) or it would create a second property and deprecate the old one and we\'d have to think about procedure when both are set and whatnot... You think a discussion on the discord would provide more insights into what others think about this whole ordeal? Would it even make a difference? The template light example is not good, that example should be for a templated number instead; maybe the example was added before we had template number. No matter that, and no matter the non-lock use case you have in mind, you\'ve convinced me that there\'s no reason to not allow templated code format , and I checked with a couple of other core developers who also agree. Can you please add negative tests which show what happens if the template has a syntax error or raises an error during rendering? Will the result be that we accept any code or no code at all or something else? Also, can you double confirm how frontend behaves when the code format attribute changes?', 'Add a check to verify the API was called to lock the lock']"
63,64,63_docstring_docstrings_docstr_doc,"['docstring', 'docstrings', 'docstr', 'doc', 'docblock', 'documenting', 'update', 'file', 'text', 'updated']","['Please update the docstring.', 'docstring', 'docstring']"
64,61,64_async_nonasync_asyncmock_hassasync,"['async', 'nonasync', 'asyncmock', 'hassasync', 'await', 'sync', 'erasync', 'callback', 'callbacks', 'delayed']","['There is nothing async here.', ""Can't this be async?"", 'Nothing async']"
65,60,65_deviceid_uuid_uuids_uniqueidrequirements,"['deviceid', 'uuid', 'uuids', 'uniqueidrequirements', 'deviceclass', 'radioid', 'devices', 'identifier', 'unique', 'device']","['Add here self. attr unique id device.unique id', ""What does this UDN look like? Are we sure its unique enough to meet the unique id requirements (I'm assuming it does because its being used elsewhere but the test doesn't show what the UDN will look like)? unique-id-requirements Ideally the test has a valid UDN diff-0ab1317eaff266adde985b400a178004e4d447a9c4adc2a653ae8c77b5c5c434R25 The UDN contains an UUID, for example: uuid:3dcc7100-f76c-11dd-87af-00226124ca30 Does uuid: need to be stripped from the string to match what get radio id() returns? Those 2 ids have sadly no relation to each other. So to summarize: - The UDN is a good unique id when discovered through SSDP - the radio-id is another good unique id, but can only be accessed after authenticating with the device. We can thus not use the radio-id as the sole unique id, because it would not allow us to differentiate two devices where we are not authenticated. - UDN and radio-id are not equal. So it's theoretically possible for a user to add the device twice: once via SSDP discovery, once via manual configuration. AFAIK, it is not possible for a config entry to have multiple unique id's? Otherwise we could also register the radio-id as a secondary unique id for SSDP discovered devices. We aren't allowed to have two different formats for unique ids for the same integration. Can the UDN be accessed with authentication as well? I triple checked if a new API call had been discovered in the past year, but sadly not. [The API does expose a UUID]( netremotesysinfodmruuid), but that one is different from the UDN returned in the context of the urn:schemas-frontier-silicon-com:undok:fsapi:1 SSDP. Wiresharking did show that that UUID is being passed in the context of the DLNA SSDP, but trying to use that will result in a very ugly brittle solution I'm afraid. Your other option is to call the api to get the radio id once its discovered via SSDP and set it. If they changed the default PIN and you can't obtain the unique id, and we don't know if we are going to present the user a flow for something that is already setup, the best option is to abort the flow as they can always set it up manually. To mitigate the cost of probing, we should abort if a config entry already exists for the host we are about to probe. OK, I've removed support for ssdp discovery of devices with nondefault PIN. The overhead of the ssdp-flow for existing devices is now 1 call to the device discovery URL to retrieve the webfsapi endpoint. This discovery file is small file ( 200 bytes):"", 'This is likely to break in the future since there is no guarentee that the mobile app will continue to use this convention. If we want to do something like this, we would need to add a published api (in another PR) to the mobile app integration to get the list of UUIDs instead so we could be sure it keeps working if the mobile app changes how it registers these in the future. I\'d pull this out of this PR for now since there are multiple new features in this PR which should be two (or more) PRs: 1. allow list for specific uuid 2. mobile app integration I\'m all for clean APIs, but is there someone willing to make a PR on mobile app and have it merged quickly? (I don\'t have the cycles) If not I suggest we keep this feature for the following reasons: - We\'d lose a good-enough fix for an ongoing problem (original issue is more than a year old) breaking functionality for many users while waiting potentially another year for the perfect fix. - We\'re not doing any crazy hacks with the internals of mobile app, just reading public entities, the risk of breaking is arguably tolerable - Even if it breaks in the future, the worst that can happen is to ignore beacons, which is exactly what happens now. So we\'d be keeping a currenlty broken system out of fear of having the exact same broken system in the future. Your call, let me know if you insist on droping this. I think bdraco is right for this specific part. We can also see translated of renamed entities that would mess with this specific ""magic"" feature. I would advise to just remove that part, the allow list is already a giant step forward and still relatively easy for end users to voluntary whitelist (allow) what they want, and finally be able to track nameless beacons. Personally, I would even remove the 40004 trick, even if it\'s already merged on mobile. I think this could cause privacy issues because this means that I can be tracked or inadvertently track other HA Mobile users when they are around my house. I am not saying this is something new, but it would become more obvious that new HA mobiles are automatically tracked. Honestly, the basic allow list is the perfect and simple solution. We just need that and documentation (web site in the sensor description) for the fact that BLE Transmitter ID needs to be manually added to HA to track the mobile app. bdraco : how do you want to proceed? Btw this PR was meant to replace the 40004 trick, but we could also combine the two if we want a no-config solution for companion users. (Concerning privacy, the real problem is the unique constant UUID, changing the minor doesn\'t make it more trackable.) Let\'s take out the mobile app async refresh companion uuids related code here and only do the allow list. Integration with mobile app can be added later if an api is merged to the mobile app. Done. Let me know if anything else is needed.']"
66,60,66_mqtt_mqttnotenabled_mqqt_mqtttemperaturecontrolentity,"['mqtt', 'mqttnotenabled', 'mqqt', 'mqtttemperaturecontrolentity', 'mqttcommandtemplate', 'mqttentity', 'inelsmqtt', 'zigbee2mqtt', 'pahomqtt', 'discoveryflowhandler']","['Do users need to have an MQTT broker setup to use this integration or do they otherwise need to know about MQTT? No, they need to log in with username and password on the ecovacs server (which is the mqtt broker and rest to mqtt converter). Only the bot speaks MQTT instead of XMPP (current integration, see comment above). Do you have a better name for it?', 'The individual component schemas change this to ""MQTT Alarm"" etc, with the intention it will take precedence. Suggest to add a new test helper help test default name in tests components mqtt test common.py call that test from each component\'s tests to verify the default name is the overridden one, and not MQTT Entity . This would be a cosmetic test though, not a functional one. Is that ok? In addition, i can remove the default value, in which case it would default to DOMAIN PLATFORM (ie. ""mqtt switch""). Oh, it does not use DOMAIN as the second part, for example alarm control panel would be just mqtt larm The way I imagine this could potentially break in the future is if the config is validated by the MQTTEntity using the MQTT ENTITY BASE SCHEMA . Hence the suggestion to add functional test help test default name(hass, mqtt mock, domain, config, expected name) which sets up an entity and tests that the default name is as expected. Edit: Maybe it\'s better to drop CONF NAME from the MQTT ENTITY BASE SCHEMA discussion r587307693 Yes, dropping CONF NAME , default naming is more sensible.', ""Why do we add this here if it's MQTT that needs it, not image itself?""]"
67,59,67_translations_device_translated_translation,"['translations', 'device', 'translated', 'translation', 'refers', 'english', 'class', 'classes', 'texts', 'greek']","['If there is a device class and no translation key or name it can take the device class translation, so if you remove translation key, it will work exactly as now, without creating more translation keys', 'Can use device class translations. So if there is no name or translation key set it will use the name of the device class', 'You could remove this one and the translation key for the device class translations']"
68,59,68_urls_urljoin_urllibparseurlencode_urlparse,"['urls', 'urljoin', 'urllibparseurlencode', 'urlparse', 'url', 'urlwith', 'assetsurl', '404s', 'deeplinked', 'encoded']","['If this is only supported to extract the URL, I think it should be url template instead', ""URLs should not be in translations, instead use a placeholder to set the URL. We do this for two reasons: 1. It prevents a URL from being translated accidentally. 2. If the URL changes, it doesn't result in the invalidation of all translations. frenck do you mind if I fix the existing url in line 6 as well or does this lead to unnecessary translation work?"", ""We should not construct URLs like this. Instead, use a library to handle that. For example, we have YARL available by default. With the context that this URL is not being fetched here, but rather is being passed to motionEye in JSON -- for motionEye to (significantly) string manipulate and then fetch: I need the query string to say URL unencoded, i.e. I need to permit raw [ {} ] characters in the result. YARL automatically encodes all query string parameters and I cannot find any way to work around that. I've very slightly modified this so I'm using urljoin rather than a naked string join, but you may still be unhappy with it. If you have an alternative library recommendation (or any other ideas), I am very happy to dig into this further. Sounds weird and all, as if this is an URL, this should be URL encoded. If motionEye can't handle that correctly; it should be reported and fixed. Constructing URLs manually like this, is IMHO something that should be avoided. That said, if there is no other way at this point Yeah. Personally, I think it's mostly okay, if we think about this not as a URL but a URL template . It's not being fetched here, and motionEye does string manipulation to this value anyway prior to fetching. From a motionEye server perspective I wouldn't expect a user to provide a URL encoded value in the UI for the webhook (esp when it will be doing string replacements on the raw value), so I think the same is reasonable for the API. Furthermore, depending on a change getting committed to motionEye itself is not practical IMO, as experience suggests that would take a very (very) long time :-(""]"
69,58,69_button_buttons_buttonentitydescription_2button,"['button', 'buttons', 'buttonentitydescription', '2button', 'buttonpy', 'click', 'pressed', 'press', 'entity', 'entities']","['This isn\'t needed, the bthome-ble package already handles this. Devices with one button event won\'t get a postfix (button) Devices with e.g. three button events will get a postfix, also the first ( button 1 , button 2 and button 3 ) if you have e.g. b"" x40 x3A x00"" , the package won\'t add a postfix -- device with one button, no postfix needed. if you have e.g. b"" x40 x3A x01 x3A x01 x3A x03"" , the package will add a postfix to all three buttons, including the first ( button 1 , button 2 and button 3 . if you have e.g. b"" x40 x3A x00 x3A x01 x3A x03"" , like in the test, the package will skip the first button ( 0x00 no event), and use postfix 2 and 3 for the 2nd and 3rd button ( button 2 and button 3 ). So, I don\'t see the need to add 1 to the postfix. Reference: Devices with multiple buttons are identified [here]( L465) The postfix is added [here]( L483), so, only for devices with multiple buttons, and also for the first button. I figured it was zero indexed.. adjusted in 58ecf7ac33b7acacd8275fa7d61877b441a7540a', 'This should be a button entity instead?', 'button']"
70,57,70_lambda_lambdas_multiline_multilines,"['lambda', 'lambdas', 'multiline', 'multilines', 'oneline', 'parentheses', 'parenthesis', 'parenthesize', 'line', 'paranethesis']","['Please wrap multiline lambdas into parentheses i have put it on one line, ok ?', 'Please avoid multiline lambdas', 'Please wrap multiline lambdas in parenthesis']"
71,56,71_init_initialisation_initializing_modify,"['init', 'initialisation', 'initializing', 'modify', 'overriding', 'mode', 'import', 'imports', 'autogenerated', 'removed']","['You can also move this above init', 'Should be done in init', ""Same here? No, this can't be in init either.""]"
72,56,72_homeassistanterror_homeassistantexceptiontemplateerror_homeassistanterrorfcommand_serviceerror,"['homeassistanterror', 'homeassistantexceptiontemplateerror', 'homeassistanterrorfcommand', 'serviceerror', 'servicevalidationerror', 'exceptionshomeassistanterror', 'valueerror', 'errors', 'reolinkerror', 'connecterror']","[""Here maybe we want to raise HomeAssistantError as it's an api call error and not a user error?"", 'Raise errors as HomeAssistantError', 'Please raise HomeAssistantError if a service call api call fails.']"
73,55,73_rebase_rebased_rebasing_changed,"['rebase', 'rebased', 'rebasing', 'changed', 'changes', 'fixed', 'modify', 'reverting', 'change', 'fix']","['I think this change is not needed - maybe a bad rebase? Yes, thanks. Fixed this.', 'Need to rebase. Some changes here are now in dev', 'My commit was lost in a rebase again.']"
74,55,74_ditto_agreed_agree_likewise,"['ditto', 'agreed', 'agree', 'likewise', 'thanks', '', '', '', '', '']","['Agreed.', 'Ditto', 'ditto']"
75,55,75_homeassistsntconst_homeassistantconstconf_const_homeassistantcomponentsrestschema,"['homeassistsntconst', 'homeassistantconstconf', 'const', 'homeassistantcomponentsrestschema', 'homeassistantconst', 'homeassistantcomponentslock', 'homeassistant', 'conf', 'homeassistantcore', 'homeassistanttype']","['You should import and use CONF API KEY constant from homeassistant.const :', 'Use CONF HOST , CONF PASSWORD , CONF USERNAME from homeassistant.const instead.', 'You should import RESOURCE SCHEMA from homeassistant.components.rest.schema and use it here. Then you have automatic access to: CONF RESOURCE , CONF AUTHENTICATION , CONF HEADERS , CONF USERNAME , CONF PASSWORD , CONF VERIFY SSL And it adds CONF RESOURCE TEMPLATE , CONF PARAMS , CONF METHOD , CONF PAYLOAD , CONF TIMEOUT']"
76,54,76_initpy_init_py_addonpy,"['initpy', 'init', 'py', 'addonpy', 'py311', 'pyvicare', 'flowpy374', 'onpy', 'python', 'pythonic']","[""Move this to init .py as you're not using it here"", 'This is only used in init .py please move it to there', ""Move this to config flow.py as it's only used there No, it is also used in the init .py file.""]"
77,53,77_unsubscribe_unsubscribed_unsubscribes_unsubscribing,"['unsubscribe', 'unsubscribed', 'unsubscribes', 'unsubscribing', 'unsubscription', 'unsubscriptions', 'unsub', 'unsubs', 'unload', 'resubscribe']","['Use entry.async on unload to unsubscribe the listener on config entry unload.', 'Use entry.async on unload to unsubscribe the listener on config entry unload.', 'Use entry.async on unload to unsubscribe the listener on entry unload.']"
78,52,78_validator_validators_validate_validation,"['validator', 'validators', 'validate', 'validation', 'validated', 'validity', 'valid', 'invalid', 'templatefileextension', 'verify']","['This should be sufficient (instead of a custom validator):', 'I think this needs a custom validator which fails if the input keys are not unique. Added validator.', 'The duplicate validator run last, so the value if present is a list. janiversen When I tested this piece of code on a real env, I obtained an error caused by the iteration over an int, so it seems to me that it run before the validation. Am I wrong? Well your validator changes the config, like some of the others, so it is important it runs after those....if it does not correct it. janiversen I spent some time using the debugger and also the other validators which change the output (i.e. number validator, nan, struct validator..) are called AFTER the duplicate entity validator. so it seems the the duplicate validator run as first, not the last. What I miss?! You miss that you need to change that. Dear janiversen I really appreciate your patience and your ( -) hidden suggestions; I tried to change the code every time you asked and I can assure that imagining what you have in mind was not so easy. But honestly speaking, this time I cannot rewrite verify the whole validation process just to avoid an ""if else"" inside a routine (we\'ll have the same for fan mode conf address of the other PR). The debugger says that it is not true that ""The duplicate validator run last [...]"" Domain level duplicate entity validator is executed BEFORE all, standard or custom field validators and in fact it is executed against fake values, if the config is wrong. I don\'t know if it is right, because IMHO it is totally unuseful to check duplicates if the entity itself is wrong, but this is not my matter and I\'m sure that who wrote that had a good reason. If you have already an idea on how we should change the whole validation process, please, explain.. Else, I commit as it is this piece of code. Its a simple problem of looking in init , not a rewrite.']"
79,52,79_function_inlinefunction_functions_inline,"['function', 'inlinefunction', 'functions', 'inline', 'inlined', 'bind', 'declare', 'linear', 'tiny', 'variables']","['this can also be a function', 'I think this function is that small that you can just inline it', 'Function is small, might as well inline it']"
80,52,80_attributeselector_attribute_attributes_getattribute,"['attributeselector', 'attribute', 'attributes', 'getattribute', 'class', 'properties', 'instance', 'property', 'constructor', 'instances']","['Set this as class attributes (outside of init ) instead of instance attributes', 'make this an instance attribute', 'All your static properties and instance attributes should be defined as class attributes Done. Moved all the static instance properties to class attributes.']"
81,51,81_asyncio_asynciowait_asyncioget_asynciocreate,"['asyncio', 'asynciowait', 'asyncioget', 'asynciocreate', 'asynciogather', 'asynciosleep0', 'asynciostreamwriter', 'asyncioloopcreate', 'async', 'asynciotimeouterror']","['We can use asyncio.gather here.', 'We can use asyncio.gather here to do this in parallel. Thanks for the suggestion, done', ""One thread per bulb is not going to scale if the user has a lot of bulbs. It also looks like the listen implementation doesn't handle short reads so it will miss any fragmented packets The protocol looks really simple. How about using asyncio.loop.create connection and buffering on data received , then you can do all your callbacks in async Thank you for the feedback, I was also wondering about the threading. I know the Xiaomi Aqara and Motion Blinds integrations also use this threading approach with one thread per gateway (not as severe), they use a socket to a UDP multicast adress. Would this same function also work for UDP multicast connections? bdraco do you have an example of a asyncio.loop.create connection implementation? I will look into it when I have some time. I'll put together a proof of concept this weekend. It may even be possible to use asyncio Streams as the protocol is very simple. Here are some UDP examples. The second one shows passing in an existing socket (which is probably what you need) L89 And passing an existing socket (note that this code should be moved out of Home Assistant core as protocol details belong in a pypi)... something on my backlog: L87 starkillerOG Functional example Example: async reader.py The proof of concept I pushed will break if you send multiple commands though before awaiting them. I'll push another commit that supports sending as many as you want (as many as the bulb can handle) at once and then sorting out the responses along with support running commands async so you can fire off a few of them and await the resposnes Cleaned it up a bit more an squashed it into bdraco Wow that is amazing, it looks like you already figured out the whole part for python-yeelight. Would you be willing to make a Pull Request to python-yeelight of your fork? Would be awesom to have those async fucntions in there. As soon as i have some time, I will start testing with your fork and addapt this PR to use those async functions. just tested it, it works like a charm :)""]"
82,51,82_idem_iden_zamg_,"['idem', 'iden', 'zamg', '', '', '', '', '', '', '']","['Idem', 'idem Done', 'Idem done']"
83,51,83_async_selfasync_setdefault_entrydataconf,"['async', 'selfasync', 'setdefault', 'entrydataconf', 'setup', 'setups', 'unload', 'reload', 'asd', 'restarted']","['You can remove async setup since', 'Move this to async setup entry so async setup can be removed.', 'Move this to before async setup entry']"
84,50,84_genericcamera_camera_jpeg_cameras,"['genericcamera', 'camera', 'jpeg', 'cameras', 'qr', 'images', 'png', 'svg', 'img', 'ffmpeg']","['Can we do SVG? I had it before, but with svg you got a problem with dark background color, because the ""white"" space between the black digits in the QR code is transparent ( will provide a screenshot today evening ) ok, let\'s keep it PNG. png svg --- --- ![image]( ![image]( qr code based on fake data', ""Should we call it image filename or would Gemini allow in the future any type of file to be uploaded? (Like OpenAI). If so, maybe we can call it file input or something. Currently only images are allowed. For any other mime type the API fails with: Failed to call service google generative ai conversation.generate content. Error generating content: 400 GenerateContentRequest.contents[0].parts[1].inline data.mime type: MIME type must be image png, image jpeg, image webp, image heic, or image heif. No idea if they plan on supporting other types. Let me know if we should go with file input to be future proof and document that only images are currently supported. Let's stick with image for now. If we allow other files, we can add a second arg. It's easier to create a selector when we allow only images."", 'We don\'t allow device and service interfacing integrations to calculate state. That\'s only allowed for template integrations and similar integrations that rely on other integrations for state. Device and service interfacing integrations should just set the state that is reported by their interface api. MartinHjelmare so does that mean the call to pyqrcode needs to be in the external library and passed into HA here, eg as an image? Or does the camera feed need to be generated there too? I hope my suggestion has not made this more complicated. My intention was the opposite It\'s ok to have the library return a camera image when requested. I don\'t understand completely what you mean with camera feed in this context. I think what MartinHjelmare means is to move generate guest wifi qr() into a separate class which lives in common.py. Did I understand correctly? No, we don\'t allow the integration to calculate the camera image. The camera image should come from the library api. If this is something that you want to add, the library needs to be updated to provide this image. Ok, now I understand. But if the project (fritzconnection) doesn\'t accept such a change (e.g. because there is a new requirement added (pyqrcode)) we could not add such a feature to Home Assistant? That\'s right? Could we go the ""service road"" (original PR) in such a case or isn\'t this allowed either? I don\'t think we should create the QR code image in the integration, either in a camera entity or service call. The idea is that integrations should express the device or service api features and not invent new features that aren\'t natively supported by the device or service api. Ok. I will close this PR until such a functionality is ready in the underlying library (which I will try to implement). FYI I started a discussion at the fritzconnection project ( MartinHjelmare kbr , the maintainer of fritzconnection , gave me the answer, that he don\'t itend to add such a functionality into fritzconnection because of the extra requirement of the QR code lib ( This means fritzconnection nor Home Assistant would like to implement such a functionality. Is there any chance to add this to HA except for a custom component? I think there may not be an obvious clean way to do this. Here are some ideas. 1. Feels like a long shot but maybe the [existing QR code HA integration]( could be somehow enhanced to not only read QR codes from video feeds but also generate qr codes from sensors, and expose them as an image somehow. But that is quite a stretch and might be best as a new integration. 2. Maybe a command line component to create the QR. 3. Maybe a separate [docker container to generate QR codes on demand]( Then you could use a jinja template to put a QR code on the HA user interface based on arbitrary text. There might be online services that will do this for free but I couldn\'t easily find any. 4. If you have a working cgi-bin locally, QR codes can be generated directly via a cgi script. [Here\'s a demo script I just made]( - feel free to use it. My idea behind doing this is to use the service to set a new guest wifi password (which I contributed here: and afterwards generate a new QR code. Every 24h! Due to this PR here it uses the underlying library to get the new password. With the suggested ideas there is no way to direct get the password. This needs extra configuration of the credentials elsewhere. If this is the way to go I would prefer writing a custom component. But what me make thinking about this is the statement from kbr in the fritzconnection discussion as he says: And I agree. I even thought HA should combine things and give new opportunities and not be limited by an underlying package....jm2c... Perhaps a good first step would be to add something that just exposed the password as a string. Then a string to QR code converter could achieve what you are looking for without having to add lots to either fritzconnection or HA. I don\'t know if it\'s a good practice to save credentials as a clear text string and make it accessible through HA frontend. If that is a concern, a QR code would open the same security hole. This is probably best discussed in the forum. But it\'s a little barrier because it\'s encrypted in an image. You must scan the code to retrieve the password.']"
85,50,85_unused_useless_removed_use,"['unused', 'useless', 'removed', 'use', 'used', 'appears', 'needed', 'nevermind', 'looks', '']","['This is unused', 'this seems to be unused?', ""This one's unused too""]"
86,50,86_stale_consistent_changed_bad,"['stale', 'consistent', 'changed', 'bad', 'good', 'vacuum', 'used', 'updated', 'constant', 'rules']","['Stale', 'stale name Done', 'stale']"
87,50,87_returns_return_returning_returned,"['returns', 'return', 'returning', 'returned', 'checking', 'retrieved', 'check', 'value', 'false', 'condition']","['Nothing is checking this return value.', 'The return value should be optional since we may return None .', 'Can this return None?']"
88,49,88_yaml_config_configuration_configurations,"['yaml', 'config', 'configuration', 'configurations', 'changes', 'env', 'pr', 'dev', 'integrations', 'integration']","['For integrations that connect to devices or services, we no longer accept new YAML configuration or changes. This integration needs to be refactored first to use a configuration flow and config entries first. More information about this can be found in Architecture Decision Record: - ADR-0010: decision See our developer documentation on how to get started creating a configuration flow for this integration: As these changes often involve a bit of work and some significant shift in the current code, we will close this PR for now. We (and our community!) would really appreciate it if you could start on the refactoring of this integration in a new PR. Thanks already! : 1:', 'For integrations that connect to devices or services, we no longer accept new YAML configuration or changes. This integration needs to be refactored first to use a configuration flow and config entries first. More information about this can be found in Architecture Decision Record: - ADR-0010: decision See our developer documentation on how to get started creating a configuration flow for this integration: As these changes often involve a bit of work and some significant shift in the current code, we will close this PR for now. We (and our community!) would really appreciate it if you could start on the refactoring of this integration in a new PR. Thanks already! : 1:', 'For integrations that connect to devices or services, we no longer accept new YAML configuration or changes. This integration needs to be refactored first to use a configuration flow and config entries first. More information about this can be found in Architecture Decision Record: - ADR-0010: decision See our developer documentation on how to get started creating a configuration flow for this integration: As these changes often involve a bit of work and some significant shift in the current code, we will close this PR for now. We (and our community!) would really appreciate it if you could start on the refactoring of this integration in a new PR. Thanks already! : 1:']"
89,49,89_ternary_ternarys_multiline_expression,"['ternary', 'ternarys', 'multiline', 'expression', 'lines', 'line', 'multilineternary', 'operator', 'spans', 'statements']","['Please make these if blocks when a ternary spans multiple lines. Thanks', 'Please split any multiline-ternary into an if block', 'This would be more readable if it was not a multiline ternary. Right, removed the ternary.']"
90,48,90_assert_asserts_asserting_assertions,"['assert', 'asserts', 'asserting', 'assertions', 'assertion', 'assertionerror', 'test', 'testcase', 'eval', 'statements']","['This is not possible so instead use assert', 'The assert is not needed here.', 'This assert should not be needed.']"
91,48,91_lowercase_uppercase_capitalized_capitalization,"['lowercase', 'uppercase', 'capitalized', 'capitalization', 'capitalize', 'renaming', 'naming', 'rename', 'renamed', 'case']","[""I'd keep the name as it's called in the library. Is there a reason to rename it? Renaming it makes it harder to look it up in the library. Please see my explanation in this PR: r1406576321 Right, but that's a problem that the library should solve. We shouldn't change the name here unless it's clearer but I don't think it is clearer."", 'I think lower is redundant here. capitalize should take care of it automatically. Nice! yes, tests pass with that.', ""Why do we capitalize the user name? It is first in the name instead of Tautulli. Should Tautulli be added in front of the username or do we just not capitalize the username? I don't think it is correct to capitalize usernames in general; they should be used as-is, because capitalization has meaning in usernames.""]"
92,47,92_deviceproperties_devicename_init_deviceinfo,"['deviceproperties', 'devicename', 'init', 'deviceinfo', 'attrs', 'attr', 'infoselfdevice', 'device', 'selfdev', 'startup']","['Use attr device info in init', 'With the change above you can move this to init with self. attr device info', 'You can set this as self. attr device info']"
93,47,93_coordinatorpy_coordinatordevice_coordinator_coordinatorasync,"['coordinatorpy', 'coordinatordevice', 'coordinator', 'coordinatorasync', 'coordinators', 'configentryerror', 'handlerspy', 'configentry', 'singletonpy', 'notifypy']","['please put the coordinator to a separate file', ""I like the idea! I don't really like the idea of splitting up the retrieving logic into the coordinator and this api class. What if in the init .py you (use a helper to) try to lookup the server type, after which we pass the MCServer to the coordinator. With as added benefit, if the server can't be looked up, we can say that the config entry is not ready. Then you can put the rest of the mapping logic in helpers to not make the coordinator one big pile of code. I think it would be nice if the coordinator shouldn't have to know what kind of server he is talking with and put those specifics away in functions, making the coordinator clean Good point, thanks : 1: I create now the API instance ( MinecraftServer ) in init .py and pass it to the coordinator. If a problem with the config entry is detected, ConfigEntryError is now raised (before, HomeAssistantError was raised instead in the coordinator). I also reached now a state, where I cannot further simplify the coordinator. It's now as simple as it can get :smile: I still stick to the server type stored in the config entry data and not to detect the server type in init .py due to the possibly long waiting times I mentioned before. BTW: ConfigEntryNotReady is already handled by coordinator.async config entry first refresh , called in async setup entry."", 'You can separate the coordinator in a coordinator.py file']"
94,46,94_snapshottesting_snapshot_snapshotassertion_snapshotting,"['snapshottesting', 'snapshot', 'snapshotassertion', 'snapshotting', 'snapshotupdate', 'snapshots', 'testing', 'test', 'tested', 'tests']","['I think we test this now with the snapshot test', 'No need to call snapshot as a function', 'You can use our new snapshot feature for this! Docs: snapshot-testing fixed to use snapshot']"
95,46,95_countdown_countdowns_timer_timers,"['countdown', 'countdowns', 'timer', 'timers', 'sensordeviceclasstimestamp', 'sensor', 'seconds', 'minutes', 'minute', 'duration']","[""What does this represent? I understand that it is a timer to turn off the fan. Based on the documentation, it differs from countdown set in that countdown is supported only by legacy versions. But does it count down during it as well? (same for countdown set ) From what I could understand, the same device will not have both functions. Legacy versions: countdown Other: countdown set That's why I implemented both. Do you think it's better to take this one off? ? I was not suggesting or implying any of that. The question is, how thus this data point behave. For example, does it count down during the count down itself as well? Based on other Tuya devices I've tested, this is just the timer setting, in this case there will be no feedback for the remaining time. Ok lets keep it for now, it matches specs."", ""This counts down the number of seconds. We don't allow that in sensors, instead calculate the datetime of when the timer finishes and use that (give it the timestamp device class as well). Additionally, this isn't a measurement? Actually it counts down the minutes left. countdown left Remaining time of countdown Integer { unit : min , min :0, max :1440, scale :0, step :1} I am not sure how to add this one correctly. frenck can you give me a hint or support on calculation of the datetime when the timer finishes? Currently I am using it as a sensor and it updates every minute until 0. Indeed it is not bad to see how much minutes are left instead of a timestamp. Calculate a datetime instead and return that :)"", 'Countdowns need to have the SensorDeviceClass.TIMESTAMP device class and provide a timestamp of when the countdown finishes. Mh... how to deal with the issue that I don\'t know seconds of the minute-based timers and the minutes and seconds of the hour-based timer? The device just gives me ""in 5 minutes"" and will change that information after one minute. If I do some datetime math and e.g. just add the minutes this would result in a value jumping back and forth: Let\'s assumed the timer was started at 20:15:25 and I just add real time timer value: Real Time Timer Value Resulting Timestamp ----- ----- ----- 20:15:25 5 20:20:25 20:16:05 5 20:21:05 20:16:25 4 20:20:25 20:17:05 4 20:21:05 20:17:25 3 20:20:25 20:18:05 3 20:21:05 20:18:25 2 20:20:25 20:19:05 2 20:21:05 20:19:25 1 20:20:25 20:20:05 1 20:21:05 This will get even worse with the hour based timer, which, set to 7 hours, will (after 59 minutes) still report 7 hours (will change to 6 after 60 minutes). Therefore, the timestamp will be wrong by one hour. This even happens if I e.g. always assume seconds (and minutes) to be 0 - either the assumed end timestamp is jumping or is incredibly unprecise. Any idea? That is actually really good reasoning, I wonder if we should make an exception for this case. MartinHjelmare May I ask for a second opinion on that one? The only other solution I can think of right now is that we don\'t update state again until the timer runs out, ie until current time passes the timestamp of the existing state. But I guess the timer can be changed during operation, so then that won\'t take effect until the old timer runs out, which is bad. Another idea: We could store the old timer value besides storing the timestamp as state. We only update state if the new timer value changes from the old timer value, eg from 5 to 4. If we then don\'t include more precision in our state timestamp than what the timer value represents, eg minutes or hours, we should be able to avoid jumps in the timestamp. Does that sound true? I see several problems when we start to try to fake the timestamp here: First, we would have to consider all the edge cases (including the one mentioned by MartinHjelmare, what happens if the timer was changed during operation). Manually decreasing the timer setting during operation is somehow easy to detect. But what happens if, e.g., after 59 minutes the hourly timer was turned off and on again (which might happen in real use). This would ""reset"" the hourly timer from 7:01 (internal value, sent by the device as 8 , since only 7:00 will change it to 7 ) to 8:00 (sent as 8 ). Our logic here could not detect that if power down up was done faster then pull interval (5 seconds), which is quite easy to do, it\'s just pressing the power button twice. Then the display in HA would show the timesamp ending 59 minutes too early. Another edge case: What happens if HA is rebooting during device hands-on actions starting or stopping or changing a timer? Or generally: Somebody uses the device control panel for changes. E.g., the physical setup I\'m going to build in my house: I have a tablet in the upstairs living room, where I can start the sauna (preheat, etc...). When it\'s fully heated, I will go down. After finishing sauna time, I directly deactivate the sauna device from a control panel which is directly in the sauna room and part of the device (water proof, so I prefer this to have in the steam cabine over a, e.g., Android tablet). Changes I do on this panel should correctly be visible to HA, e.g. if I go upstairs I want to check if everything (fan) turned off after a while, when the sauna is assumed to be cold and dry again. Same for the hot water tank: It looks crazy if the one sensor (water tank level) shows ""empty"", while the Home Assistant timer is saying ""I will empty that in 31 minutes and 45 seconds (this is part of the meaning of the hourly sweep timer). I\'m sure there are pretty more edge cases which we didn\'t discuss or think about yet. Furthermore, testing is quite heavy with an hour based timer and a device with 7.5kW power consumption... :laughing: So figuring out bugs could take a long time which I then would better need for paid work for the incoming energy bill. :laughing: Currently, the whole tolo integration (regarding device communication) is completely stateless. The device\'s state is reflected in 25 bytes (17 status, 8 settings, see which is received by the tololib dependency and after conversion to native Python types directly passed to and displayed by HA. Going the way of faking a value which is actually not there increases the total complexity of the integration and therefore increases the difficulty of maintenance, too. I would strongly vote for an exception in this case and just displaying the original value in a sensor as it currently done in the PR. My best and favourite argument: By convention, you want to show a timestamp (as it gives best precision, which I completely understand). But faking a precision which we don\'t have with heavy and most probably buggy (e.g. not considering all edge cases) logic and then probably showing the user wrong in information in high precision instead of showing the correct information in the original lower precision... it just feels quite wrong and, IMHO, shouldn\'t be the goal of this convention. You\'re first problem example is due to the involved precision from the API. If the API doesn\'t give better precision than hours the state can indeed be minutes off. I don\'t follow your second paragraph. Sorry. We want to use a timestamp as state instead of relative time when the relative time changes just because time passes. We don\'t want our state to change just because time passes. I think it\'s ok to use a relative time, as an exception here, if we don\'t update state more frequently than once per minute. Currently, we update all states of the tolo integration (e.g., temperature, water tank level, water tank temperature, ...) on a 5 second basis (since temperatures can actually change quite fast), sending one status request message and getting back a 17 byte UDP package, containing all the information, including the information we\'re discussing here. For reasons of simplicity, I would just return this value (as already coded in the pull request). Then, theoretically, it can get updated more frequently than once per minute. To be clear: Merging the pull request as it is does not increase network traffic polls ..., it just returns a value which is already known to the coordinator, since one status message contains all the information. There would be a specific request response possibilities for 3 of 4 of the possible timers, only sending back a single byte (plus overhead), but I would not use it since the information is already there in the coordinator. Alternatively, we decrease the the local pull interval for the whole integration, which significantly decreases UX (temperature values are quite relevant for the integration and may increase mich faster than one minute can reflect and might be useful to the user). E.g., with the 5 seconds update interval, I can see if the water tank is getting refilled (completely automatically), so the temperature decreases slightly, I can see the heater starting again, bringing the temperature up to 100 , all happening within 2-15 seconds (depending on the water inbound temperature). So, for UX, in my opinion not a good choice and significally reduces usefulness of the integration. Third option: In the getter, add new code which ""caches"" the last reply until one minute has passed and then use the actual value already known to the coordinator - and this does not make any sense I guess. Just for understanding... state change is different from returning a value for the sensor , right? A state change only occurs if the value getter returns something different to its previous call, right? If yes: Forget about my previous comment, then it\'s already implemented that way. Except heavy strange behavior of people administrating the device (turning it on off all the time or changing timer values all the time), the return value of the value getter for timer sensor entities will only change once per minute. The state won\'t be updated in the state machine unless the state or state attributes change, unless the entity has set force update to True. Then... it\'s all set. I don\'t play around with force update at all and (except of strange user behaviour) the value stays the same for 1 minute. And if you would be willing to accept the exception for this sensors to allow int (minutes) instead of timestamp for the timer... everything is implemented. Thanks for the discussion MartinHjelmare and frenck! Personal note: Sorry if I\'m annoying sometimes, I just want to get the best outcome for HA users of my integration, but I also have to learn a lot about HA, so I ask a lot of questions to really understand things... We have a common goal As there are no other attributes, and all units are minutes, and the native type is an integer. I think this is OK in that case, as we agreed to allow for it. Thank you very much!']"
96,46,96_strings_string_breaking_break,"['strings', 'string', 'breaking', 'break', 'lines', 'multiline', 'longer', 'length', 'shorter', 'line']","['Please break long strings around max 88 characters per line.', 'Please break long strings around max 88 characters per line.', 'Please break long strings around max 88 characters per line.']"
97,46,97____,"['', '', '', '', '', '', '', '', '', '']","['Same as above', 'Same as above', 'Same as above']"
98,45,98_const_consts_constnats_cluter,"['const', 'consts', 'constnats', 'cluter', 'underscore', 'variables', 'prefixed', 'homeassistantconstconf', 'standard', 'janiversen']","['Please move this out of const', ""there's a const for that"", 'Move these to .const?']"
99,45,99_searching_search_linear_optimized,"['searching', 'search', 'linear', 'optimized', 'searches', 'lookups', 'sets', 'lists', 'list', 'lookup']","['Is there a way we can avoid this linear search? This is going to get called for every zha row. If they have a ton of events we potentially have a large loop inside of a large loop which could be a scale issue I could drop the translation to the device trigger information? I think that would be slightly confusing to the users though. The problem is that I am using the trigger schemas to determine if there is a match. I may have an idea that could make it a bit better. Stay tuned. Thanks again for the assist!', 'Can this linear search be avoided? What about: There is always an attributes property, so we can remove the 1st condition. Applied your new code statement, however it is still some kind of linear search, just a bit more optimized.. Ideally this gets optimized in the future, but its ok for now', 'a set would have been better here to avoid linear search.']"
100,45,100_async_selfasync_superasync_entityasync,"['async', 'selfasync', 'superasync', 'entityasync', 'sync', 'writes', 'callback', 'write', 'state', 'statetrue']","['Did you intent to call self.async write ha state() as well here to update the state machine?', 'Did you intent to call self.async write ha state() as well here to update the state machine?', 'If its async you can use async write ha state']"
101,44,101_port_ports_selfport_cvport,"['port', 'ports', 'selfport', 'cvport', 'localhost', 'http', 'host', 'ip', 'input', 'config']","['data.get(CONF PORT, DEFAULT HTTP PORT) this repeats 3 times in this PR, can we make it a method that receives the data dict and return the port? get http port', ""How should the user know that they can enter port by using a colon in the host form? Normally the form name or description should inform the user what data should and can be entered. Do we have any precedence where the host form is used to enter both host and port? Please use lowercase snake case variable names. I updated the description for the host. We should also look at precedence. In the same commit, I updated variable names from hSplit to parts in two places. I was referring to this question: Yes, this is a well-known notation for specifying port in URLs: : :text Port 20numbers 20are 20sometimes 20seen,8080 20of 20the 20HTTP 20server. Right, but I mean in Home Assistant config flows. I am not aware of any precedence in Home Assistant. However in this case the use case for the user is exactly the same as in a browser. In a browser, there is a default port 80 and you don't have to specify it. Most non-dev people are not even aware of the possibility to specify a port in a browser. Similarly, in this case, most people will not use the port and it makes total sense not to put it in a separate field. I don't agree. We shouldn't introduce a new pattern in the config flow forms for a single integration. We should follow the common patterns in Home Assistant so that config forms are as familiar as possible between integrations. The only form item in Home Assistant that I'm aware accepting both host and port is the url form. If that doesn't fit this case, I suggest using separate forms for host and port. Side note: Please don't resolve the conversation if there's disagreement in the conclusion. I don't agree with you. Are you a gatekeeper for merging this PR? Can I disagree with you and still move forward with merging this PR? Should we wait for rytilahti to chime in on the subject? What if rytilahti does not agree with you either? Yes, it currently blocks this PR from being merged. Codeowners should also weigh in. If members disagree we'll need to find concensus among further members. I agree with Martin, this is not a good way to handle this input and also not the way this is commonly handled in Home Assistant either. Exactly that. Hey just curious as a user of tplink, how would you be able to change the port that these devices use? I thought they were hard coded to use port 9999"", ""We shouldn't store the port in the host config entry item and probably not ask for port in the same form item as host either. If any string splitting should be done it should be done in the config flow, since for the device api host doesn't include the port. Sometimes in integrations we ask for a url which can contain both host and port eg. So there may be some leeway in the form to ask for host and port combined. Codeowners should decide how they want the form. Thank you for your guidance. I modified the config flow. I still want to do host splitting, since most people will not specify a port, and the extra port field in UI will be confusing for them. I discussed that with Codeowners in python-kasa when I created PR to support passing a custom port.""]"
102,44,102_flowtest_abortflow_flowresulttypeabort_mockconfigentry,"['flowtest', 'abortflow', 'flowresulttypeabort', 'mockconfigentry', 'flowresulttypecreate', 'flowunknownflow', 'testscomponentsairqtest', 'tests', 'failed', 'config']","['Please finish config flows in either Create entry or abort so we also test that the config flow can recover from an error', 'Config flow tests should end with ABORT or CREATE ENTRY , this way we can check that the config flow can recover from a error', ""These failure tests should end with abort as in a successful reconfigure. Why should they be a successful reconfiguration when the user supplied wrong information? It should not. It should continue by the user setting the right information within the same flow eg the user try again. The current behavior is that if the user enters wrong information (e.g. api wrong) or the api is unresponsive he will be presented with the same flow again and the error is displayed. Do you rather want that the flow gets aborted and the user has to click Reconfigure on the ConfigEntry again? No Let's days the user enter the wrong credentials he should be presented with the form again (with an error). Then he enters the right ones and then it's success. Same for if there is a server problem or some other exception. So the same already started flow should continue (after whatever exception) and then end with an abort with the reason reconfigure succesful That is how it currently works Do you want me to test the successful case after a failure again? This test and below tests are all ending with a form which is presented to the user with an error. I want all these tests to continue from that point with the user entering data again and then it's successful eg it ends with an abort All config entry tests should end with either abort or create entry (so the tests above these are actually also not correct). Will provide some references tomorrow if still unclear what I mean. Okay now I am sure I understand what you want me to do. I still don't understand why ? This is the first I am hearing or seeing any of this and explains my confusion. What is the reasoning behind this rule? If I look at the code for this integration I see 3 scenarios to be tested: 1. The user is presented a form is where information can be entered. 2. When the user enters correct data in the form and no errors are encountered the user gets the info that create reconfigure was successful 3. When the user enters incorrect data or an error is encountered, the form is displayed again with an error description. From my point of view what you want to test is a combination of scenario 1- 3- 2. We test each scenario separately. As there is not internal state between the scenarios I do not see why we would need to test the whole roundtrip. What am I missing? We should test that for any type of exception the flow can recover from those exceptions and continue until either an entry is created or we abort the flow. I would have been glad if you had pointed why this is a missing scenario in my comment above. I will add additional test cases to get this merged.... I'm not sure I understand what you mean. In the test cases with an exception you end them with a form which means the flows are still open. I don't want to have more tests, I want the current tests extended so they finish properly. We want such test cases to continue from that point so we remove the exception, allow the user to again enter the form data so it can end in a success ( create entry or abort ). By this the flows ends properly and we also see that a config flow can recover from exceptions so a user would not have to restart a flow but can simply again put in form data and press submit. [Here]( L63-L122) is an example what I mean by a recover from a failure so in this case it ends with create entry Extended all failure config flow tests.""]"
103,43,103_pylint_pylintdisable_pylib_pymodbus,"['pylint', 'pylintdisable', 'pylib', 'pymodbus', 'pylance', 'importerror', 'unusedimport', 'pip', 'contextlib', 'import']","['No need to disable this pylint check', 'Fixed in pylint 2.7.3.', 'Fixed in pylint 2.7.3']"
104,43,104_errors_error_errorsbase_fails,"['errors', 'error', 'errorsbase', 'fails', 'failures', 'validation', 'inconsistent', 'sfrboxerror', 'removed', 'occurred']","['errors removed', 'Errors is always set', 'Should this be an error?']"
105,42,105_return_type_types_resultresponse,"['return', 'type', 'types', 'resultresponse', 'typed', 'method', 'result', 'specify', 'typing', 'validate']","['Please add a return type', 'Return type', 'Return type Done']"
106,42,106_use_used_using_obsolete,"['use', 'used', 'using', 'obsolete', 'isnt', 'anymore', 'appears', 'remove', 'version', 'doesnt']","['Seems not to be used?', 'This is not used', 'This is not used.']"
107,42,107_mypy_switchpy_empy_selectpy117,"['mypy', 'switchpy', 'empy', 'selectpy117', 'dtpy51', 'pytyped', 'simplisafepython', 'python', 'pythonbinance', 'pep0484']","['mypy does not agree with the changes', 'If this is only here to make mypy happy, do like this instead:', ""Looks like mypy can't figure it out without it. Without this line mypy gives me those errors:""]"
108,42,108_necessary_required_needed_need,"['necessary', 'required', 'needed', 'need', 'anymore', 'longer', '', '', '', '']","['not needed', 'Not needed', 'not needed']"
109,42,109_constpy_const_constants_constant,"['constpy', 'const', 'constants', 'constant', 'consts', 'conftestpy', 'conf', 'import', 'pywmspro', 'declare']","['Any of these constants that are defined identically in multiple files should be added to a const file and then imported. TYPE , ID , PROPERTY NAME , PROPERTY VALUE , etc', ""I think this might be causing the circular import. Normal procedure is to create the shared constants in const.py then import those elsewhere. I don't think we should import consts from other files back into const.py"", 'You defined constants in const.py : DEFAULT HOST, DEFAULT USERNAME, what is the use for them? Now I use those constants, done!']"
110,42,110_checks_checking_checked_check,"['checks', 'checking', 'checked', 'check', 'checkin', 'optional', 'remove', 'false', 'removed', 'value']","['Why you need this check?', 'Do we need this check?', 'Move this check up before we do anything else.']"
111,42,111_writingtestsforintegrations_tests_testing_test,"['writingtestsforintegrations', 'tests', 'testing', 'test', 'mock', 'assert', 'integrations', 'integration', 'diagnosticsdata', 'asserting']","[""We don't want to access hass.data in tests since thats considered integration details. patch the library instead and call async fire time changed writing-tests-for-integrations"", ""Please don't access integration details like the coordinator in the tests. We only want to assert core state. writing-tests-for-integrations"", 'We should not access integration details in the tests. writing-tests-for-integrations']"
112,41,112_humidity_humidifying_humidifier_humidifiers,"['humidity', 'humidifying', 'humidifier', 'humidifiers', 'dehumidifier', 'dehumidify', 'climateentityfeaturetarget', 'dehumidifying', 'airzoneentity', 'configget']","[""Let's remove this, conversion to and from a scale other than humidity in can be implemented via the target humidity command template and target humidity state template instead. Remove this functionality"", 'At self. attr target humidity None at self. init () instead if CONF HUMIDITY STATE TOPIC is None', 'Do we not need a CONF HUMIDITY COMMAND TOPIC too? Further it would be good to add CONF MIN HUMIDITY and CONF MAX HUMIDITY as well.']"
113,41,113_fields_omitted_remove_unused,"['fields', 'omitted', 'remove', 'unused', 'omit', 'drop', 'cleaned', 'format', 'removed', 'values']","['Please remove empty fields', 'Please remove empty fields', 'Remove empty fields']"
114,41,114_lists_comprehensions_comprehension_list,"['lists', 'comprehensions', 'comprehension', 'list', 'dict', 'python', 'iterate', 'loops', 'iterations', 'iteration']","['You can use a list comprehension for this one.', 'This could become a list comprehension', 'This could be a list comprehension']"
115,40,115_bluetooth_bluetoothserviceinfobleak_ble_bt,"['bluetooth', 'bluetoothserviceinfobleak', 'ble', 'bt', 'bthome', 'deviceaddress', 'devices', 'xiaomi', 'device', 'connectable']","['Do not hold on to the ble device . [See the Bluetooth docs for best practices](', 'There is still some use of ""august"" in the test fixtures. This is expected as the yale (company) code was forked from the original august code and they still have august all over the place internally. Even the yale locks you buy have august in the bluetooth firmware (although there are no references on the packaging or device) Its all super confusing. I\'ll try to summarize what I know but even some of this is based on discovery and patchwork information as even the people working at entities themselves don\'t seem to have the full story. August was sold to [ASSA ABLOY]( and than the North American devices were spun off into yet another company ""Yale August"" aka august . - The original august system is ""dead"". - The yale integration will be for what the ""ASSA ABLOY"" company is doing. aka yale fork aka outside of North America. ""ASSA ABLOY"" is moving most all of their existing integrations to this platform. Yale Doorman and other similar devices will only eventually be supported here. Push updates are currently broken here because the api is now websocket based. - The august integration will be for what the ""Yale August"" company is doing. aka august fork aka North America. Assure2 locks and the Assume locks will only be supported here. The original August company devices are folding into Yale August . The august company is keeping the current pubnub based system for integrations at this time. They may move to another system but thats unknown at this point.', 'See the xiaomi ble config flow tests for all the cases that we normally test for bluetooth flows. Eg: L732-L733 L801-L802 L820-L823']"
116,40,116_dataupdatecoordinator_settingdataupdatecoordinator_selectdataupdatecoordinator_timestampdataupdatecoordinator,"['dataupdatecoordinator', 'settingdataupdatecoordinator', 'selectdataupdatecoordinator', 'timestampdataupdatecoordinator', 'plenticoreselectupdatecoordinator', 'plenticoreupdatecoordinator', 'tedupdatecoordinator', 'updates', 'async', 'coordiantor']","['This is already part of the constructor of the DataUpdateCoordinator', 'Not needed since using DataUpdateCoordinator', 'Why not use a DataUpdateCoordinator ?']"
117,38,117_updatefailed_updates_update_raise,"['updatefailed', 'updates', 'update', 'raise', 'raising', 'status', 'implemented', 'fetch', 'fetching', 'fixing']","['Raise UpdateFailed directly and drop the try except', 'We want to raise UpdateFailed in this case.', 'raise UpdateFailed']"
118,37,118_entitypy_entity_entities_climatepy,"['entitypy', 'entity', 'entities', 'climatepy', 'ecowittentity', 'subclass', 'climate', 'classes', 'py', 'namespace']","[""I would move EcowittEntity to a separate module. Should this one really be in a separate file, the base entity in this case is so simple. Is the preferred structure now to have a separate file? It's not compulsory, but it seems to always start off simple - and then somehow just seems to grow as we add platforms or features... (I have to say that attr and EntityDescription have helped quite a bit with that) It's common practice to put base entity definitions in init .py . I think this is okay."", 'Is it allowed to have multiple classes in a subfolder? Here we just have 3 classes (base, awning, vertical), however later on for climate devices we will have 10 implementations. Could it be that Error: homeassistant components overkiz cover entities vertical cover.py:42: error: Class cannot subclass ""OverkizGenericCover"" (has type ""Any"") [misc] is related to this? Try adding an init .py in the subfolder', ""This code looks very similar to async set ac state property in climate.py . In a future PR it would be good to make it a bit more DRY (maybe add a base class for them to share) and reuse your new exception constant there. As also more service calls will be added that sounds like a good idea. Probably will make it when I'm going to make a base class for the entities. But then this PR needs to go in first.""]"
119,37,119_coverage_coveragerc_covered_covering,"['coverage', 'coveragerc', 'covered', 'covering', 'missing', 'cover', 'incomplete', 'exclude', 'bug', 'uncovered']","['Missing coverage here', 'Missing coverage', 'Missing coverage here']"
120,37,120_r892030842_r821453593_r821451935_r1083345302,"['r892030842', 'r821453593', 'r821451935', 'r1083345302', 'r841228316', 'r901160347', 'r1015786246', 'r887550614', 'r805184684', 'r872898333']","['r892030842', 'r892030842', 'r892030842']"
121,36,121_does_works_body_work,"['does', 'works', 'body', 'work', 'watchdog', 'whats', 'difference', 'case', '', '']","['What does this do?', 'What does this do?', 'What does this do?']"
122,36,122_loggerexception_exception_logging_debugging,"['loggerexception', 'exception', 'logging', 'debugging', 'debug', 'exceptions', 'logs', 'log', 'trace', 'error']","[""For unknown error I'd log the exception stack trace at debug level. Pass exc info True in the debug log call."", ""If we're raising an exception we shouldn't also log the stack trace. Include the log message in the exception argument instead."", ""Why log the exception stack trace here? It's an expected exception and no need to log stack.""]"
123,35,123_casttypet_casting_casts_caststr,"['casttypet', 'casting', 'casts', 'caststr', 'cast', 'typevar', 'type', 'generic', 'typed', 'instantiate']","['added the import for cast too', 'This needs to be cast', 'We should probably cast here instead']"
124,35,124_timeout_timeouts_timeouterror_seconds,"['timeout', 'timeouts', 'timeouterror', 'seconds', 'minutes', '10ms', '100ms', 'exits', 'unavailable', 'patience']","['What is this timeout exactly? This is the timeout period used for device communication.If the device does not return data within this timeout period, an exception will be thrown.', 'Does this method have a built-in timeout? If not: wrap in a timeout. If so: how does the timeout communicated ? Added timeout in the lib', 'What about a timeout ?']"
125,35,125_coroutine_coroutines_coroutineany_callback,"['coroutine', 'coroutines', 'coroutineany', 'callback', 'await', 'awaitable', 'callbacks', 'async', 'awaitabledevice', 'awaitablenone']","[""A coroutine function can't be a callback. If we don't need to await inside this can just be a callback."", 'A coroutine function is not a callback.', 'A coroutine function is never a callback.']"
126,35,126_docstring_docstrings_stale_romybinarysensor,"['docstring', 'docstrings', 'stale', 'romybinarysensor', 'issue', 'fix', 'initialize', 'just', 'romysensor', 'changed']","['stale docstring Done', 'stale docstring done in', 'Stale docstring?']"
127,35,127_line_newline_lines_blank,"['line', 'newline', 'lines', 'blank', 'formatting', 'oneliner', 'end', 'remove', 'code', 'missing']","['blank new line at the end', 'this line and the next must be removed', 'Remove empty line.']"
128,35,128_indent_indents_indentation_unindent,"['indent', 'indents', 'indentation', 'unindent', 'unindented', 'condition', 'outdenting', 'conditions', 'save', 'outdent']","['You can save some indent by reversing the condition and return, then outdent below', 'You can save some indent by reversing the condition and return, then outdent below', 'You could save some indent by reversing the condition and continue']"
129,34,129_metering_sensor_meter_sensors,"['metering', 'sensor', 'meter', 'sensors', 'measuring', 'counters', 'periodically', 'longtermstatistics', 'increasing', 'statistics']","['Shouldn\'t this be a total instead of a measurement? Honestly I\'m not sure. Accordind to the documentation, total ""state class should not be used for sensors where the absolute value is interesting instead of the accumulated growth or decline"". I think the absolute value is interesting for these sensors. What is your opinion? IMHO this is a total increasing sensor. emontnemery ? Value may drop to zero after log reset. The state class impacts what statistics is created. If state class is ""measurement"", min, max and mean statistics is created every 5 minutes. If state class is ""total"" or ""total increasing"", a total sum is updated every 5 minutes. I did a little test, changed the log retention to 1h in the NextDNS configuration and found the API returns values that can go up and down so total increasing doesn\'t fit here. ![image]( I\'m confused Should I use total or measurement ? That graph is not using long-term statistics and does not show the effect of setting total increasing . Use a statistics cards instead. Looking at the graph value, the value seem to be a rolling window? It looks like this is a rolling window and the log retention is taken as the time, unfortunately the API documentation is silent about this. So, what is this value then? For example, if log retention is set to 1 day the value is the sum o queries from the last 24 hours. This is my assumption, but it fit to my observation. So, you\'re right, SensorStateClass.TOTAL should be used here. ![image](', 'Energy should have state class total or total increasing. long-term-statistics', 'I expect this measurement does not periodically reset? If that\'s the case, set state class to total instead; how-to-choose-state class-and-last reset Same comment for the other measurements. Interesting,.. the part that you link to, states about total : ""The sensor\'s value never resets, e.g. a lifetime total energy consumption or production: state class total"" But a few paragraphs above, total increasing is explained as: ""Similar to total, with the restriction that the state represents a monotonically increasing positive total, e.g. a daily amount of consumed gas, weekly water consumption or lifetime energy consumption ."" So I\'m guessing this has changed and documentation is not yet up to par?']"
130,34,130_guard_guards_guarding_adguard,"['guard', 'guards', 'guarding', 'adguard', 'removing', 'remove', 'removed', 'handled', 'drop', 'replace']","['Same here, please remove the guard.', 'Same here, remove the guard', 'We have code to guard for None . Should this be?']"
131,33,131_warnings_warning_warn_logging,"['warnings', 'warning', 'warn', 'logging', 'loggererror', 'debug', 'log', 'errors', 'error', 'notification']","['return after logging the warning instead.', 'This should not be a warning but debug.', ""Add a new abort reason and put this message in there. That way it's visible to users in the UI . Done. We can remove this warning being logged. It's already shown to the user. Warning removed.""]"
132,33,132_bc24777_fixed_cfb5fe1a8228c704a88f7e4ae9f9a58dd7f2d6d4_6cd945738e66a249e8691c243d817e869028b682,"['bc24777', 'fixed', 'cfb5fe1a8228c704a88f7e4ae9f9a58dd7f2d6d4', '6cd945738e66a249e8691c243d817e869028b682', '485ef31a8e37311bcba90f72127fb857f8695aeb', '8b31006d58c8579f4f6addec4b4b8f3a5ab28487', 'b945a09e690a5c995742befbb4806d31da51a6ca', 'in73f4e1c', '3818a8592f1ac1a1de1f58b0bf0044734b61e882', '1bb070f']","['Fixed in bc24777', 'Fixed in bc24777', 'Fixed in bc24777']"
133,33,133_pypi_pyprojecttoml_pypiorg_pynanoleaf,"['pypi', 'pyprojecttoml', 'pypiorg', 'pynanoleaf', 'package', 'pysqueezebox', 'pkg', 'pythonglancesapi', 'setuppy', 'psutil']","['This is code that should be wrapped in a library hosted on PyPi. See: I have make a python package on PyPi', 'should probably also move to pypi package', 'This should be in your pypi package']"
134,32,134_ssl_sslsslerror_sslsslcontext_sslcertverificationerror,"['ssl', 'sslsslerror', 'sslsslcontext', 'sslcertverificationerror', 'https', 'certificate', 'sslcipherlist', 'cert', 'exceptionsclientconnectorcertificateerror', 'clientsidecertificates']","['Please use CONF VERIFY SSL', ""Can't verify ssl for http : )"", 'If the SSL settings are incorrect, what error will be raised when calling fully.getDeviceInfo below? If use ssl is wrong, meaning that it is set to True but the Fully app is not configured for SSL, the error will be ""Failed to connect"" (""cannot connect"" with the translations folder removed) regardless of the state of verify ssl . If use ssl is correct, meaning that it is set to True and the Fully app is configured for SSL; but verify ssl is wrong, meaning that HA has not been loaded with the CA to verify the certificate, there will be an SSL error from the backend library. OK, and how is that shown to the user? On the frontend the user gets ""Failed to connect"" as caught by except ( ClientConnectorError, FullyKioskError, asyncio.TimeoutError, ) as error: ![image]( And the debug log registers: aiohttp.client exceptions.ClientConnectorCertificateError: Cannot connect to host 192.168.1.30:2323 ssl:True [SSLCertVerificationError: (1, \'[SSL: CERTIFICATE VERIFY FAILED] certificate verify failed: unable to get local issuer certificate ( ssl.c:997)\')] OK, in that case I\'d suggest to set the error to something more detailed than cannot connect to help the user distinguish between the different errors. That was my intention originally but there\'s nothing for ssl errors in homeassistant strings.json. I guess the general expectation is that the user will check the logs for the specific failure when getting ""Failed to connect"", specially if he is advanced enough to be setting up ssl with the integration. emontnemery, thoughts? I\'d suggest to include the the exception error message in the description placeholders since the error message is likely to aid the user if they made a mistake in the SSL configuration. Now the exception message is shown to the user on the frontend, as recommended. ![image](']"
135,32,135_selectselector_selectselectormode_selectselectorconfig_selector,"['selectselector', 'selectselectormode', 'selectselectorconfig', 'selector', 'select', 'selected', 'selectors', 'multiselect', 'selectentitydescription', 'selectselectorconfigsort']","['Please use a SelectSelector instead', 'I think we can make this a select selector since we already know all possible values Done', 'Use a select selector instead, maybe something like:']"
136,32,136_4communicationwithdevicesservices_5communicationwithdevicesservices_protocol_pypi,"['4communicationwithdevicesservices', '5communicationwithdevicesservices', 'protocol', 'pypi', 'interface', 'udp', 'documentation', 'rpa', 'tcp', 'library']","[""This integration has existing code that has too much protocol details. We won't allow new code that introduces more protocol details. For more sensor types to be added we need to first extract the device specific interface code and data parsing to a standalone library published on PyPI. I just now saw your comment in another PR adding the same feature... Should have looked into the closed ones as well. I see your point to not include protocol specific code, but it is quite bad we cannot integrate any non-existing enocean hardware for now. Do you know if someone is already working on a new library? [The one we are using right now]( seems to be abandoned anyway. I don't know of any progress in this direction lately, unfortunately. There's an Enocean Discord server for talking about the integration but there's not much activity."", 'Please extract the device specific interface code and data parsing to a standalone library published on PyPI. 4-communication-with-devicesservices', 'Please extract the device specific interface code and data parsing to a standalone library published on PyPI. 4-communication-with-devicesservices']"
137,32,137_entitycategory_entitycategoryconfig_entitycategorydiagnostic_categories,"['entitycategory', 'entitycategoryconfig', 'entitycategorydiagnostic', 'categories', 'category', 'entities', 'entity', 'entitymetadata', 'featcategory', 'categorization']","['Both entities should have the category configuration', 'This should be ENTITY CATEGORY DIAGNOSTIC', 'Entity category should be set to CONFIG']"
138,32,138_property_properties_removing_removes,"['property', 'properties', 'removing', 'removes', 'remove', 'change', 'removed', 'useless', 'unused', 'allows']","['Remove this property too.', 'remove this property', 'We can remove this property.']"
139,32,139_annotations_annotate_annotation_annotated,"['annotations', 'annotate', 'annotation', 'annotated', 'type', 'types', 'typed', 'untyped', 'typingfinal', 'signature']","['Please type the whole signature when adding type annotations.', 'Missing type annotations. Please type the whole signature when adding type annotations.', 'The parameter is missing a type annotation. Please type the whole signature when adding type annotations.']"
140,31,140_step_steps_await_async,"['step', 'steps', 'await', 'async', 'form', 'forms', 'selfasync', 'follow', 'formstep', 'prompt']","[""Don't present the form of another step, instead call async step user"", ""We're potentially moving directly to the user step form from here. We shouldn't do that. Each form is private to its step method. Await the step method to change step instead."", 'There is a design standard that you have to show the form for the step in the step itself. This form should move to step async step location , and instead here invoke return await self.async step location()']"
141,31,141_domain_domains_unsupported_config,"['domain', 'domains', 'unsupported', 'config', 'belongs', 'issue', 'brackets', 'standards', 'str', 'prefix']","['Type domain as domain: str list[str]', 'This is not the documentation for the correct domain', 'please import the TODO domain as TODO DOMAIN Like, they are the same string, but we should use the domain here']"
142,31,142_constructor_constrcutor_set_init,"['constructor', 'constrcutor', 'set', 'init', 'outside', 'static', 'existing', 'inits', 'opposite', 'insteon']","['Can be set outside of the constructor', 'Can be set outside of the constructor', 'Can be set outside of the constructor']"
143,31,143_uniqueidrequirements_unique_id_ids,"['uniqueidrequirements', 'unique', 'id', 'ids', 'attr', 'init', 'infoid', 'attribute', 'assign', 'assigning']","['Set this as self. attr unique id instead', 'This can be set as self. attr unique id', 'Could you use attr unique id ? Done']"
144,31,144_duplicated_duplicate_code_coding,"['duplicated', 'duplicate', 'code', 'coding', 'hardcoded', 'hardcode', 'copy', 'script', 'repetition', 'merge']","['maybe put that in a method to avoid duplicate code', 'duplicate code as above', 'Duplicate code.']"
145,30,145_runtimedata_entryruntimedata_runtime_entrydata,"['runtimedata', 'entryruntimedata', 'runtime', 'entrydata', 'entryruntime', 'entrydatahost', 'entrydatagetconf', 'dataclass', 'data', 'entries']","['Could this use entry.runtime data instead?', 'Please use [runtime data](', 'Use runtime data instead']"
146,30,146_hardwareaddress_generaldiagnosticsstructsnetworkinterfacehardwareaddress_mac_address,"['hardwareaddress', 'generaldiagnosticsstructsnetworkinterfacehardwareaddress', 'mac', 'address', 'addresses', 'registryformat', '802154', 'ip', 'registry', 'hostname']","[""Maybe it's better to use homeassistant.helpers.device registry.format mac to enforce a common format for the mac address? Disregard, I didn't realize mac code is just 4 characters and not the complete mac address."", 'If the mac address is available, its generally preferred to set connections over identifiers Unfortunately the Kaleidescape API does not expose the mac address.', 'Is this a mac address? If so, you should add this:']"
147,30,147_min_minvalue_minimum_max,"['min', 'minvalue', 'minimum', 'max', 'threshold', 'thresholds', 'maxresults', 'values', 'emaximum', 'value']","['Min max?', 'Same as min value, please just use max value and remove max value threshold. same issue as min value', 'It would be a lot easier to understand if you just use min value. something like min value on its own sets a value it cannot be smaller than (and the way it is coded you can do exectly that with just min value) BUT ... This will not work for my use case ... I would have to add another setting i.e. how do you suggest we achieve: if it is less than 0.025 mA, treat it as 0 I am open to another suggestion, as min value on its own will not. BTW.. reviewing this myself, these 2 very similar bits of new code should be moved into a separate function process result that scales, offsets, and checks min max Well you started off by suggesting only a min value ? My biggest problem is that you add 2 keywords (4 in total min max), and do not cover the cases where only 1 keywords is used (e.g. min with threshold or just threshold). This is adding a complexity to the configuration that is unwanted. I have no problem with a simple filter function (min max) but extending it is really something you should do with a template sensor, where you have a lot more possibilities. It is important to keep the configuration as simple as possible, the modbus integration is responsible for communicating with modbus devices and does very simple value manipulation. Just to give you an idea we have had requests to replace ""scale"" with a function, allowing the value to be graduated instead of a simple scaling, the list manipulation wishes are big, and are all solved with the template sensor. I started with one parameter. You asked me to add min value and max value which I did. If you don\'t use them it will work as now. If you just specify min value or max value then the pr will work as you requested. I had to add optional thresholds to make them work for my original use case. Please check the comments on the pr itself as they clearly indicate you can just have min value and it will work as you expect. This is clear in the code where the values are loaded in the constructor BTW.. my original proposal had been to clamp small values near 0 to 0, but what is there now is more flexible and will handle your use cases as well yes, and if you use a template sensor, you do not need to make a custom integration, which you need to keep updated whenever the core modbus integration changes pymodbus version. And not to forget my comment about what happens if you use just min value (which will not work with this PR) or if you use just min threshold which might have some side-effects. I tested min value on own and it works as expected. If you specify min value threshold on own it will do nothing. These cases are covered by the code']"
148,29,148_debugging_debugger_debug_print,"['debugging', 'debugger', 'debug', 'print', 'printing', 'breakpoint', 'leftover', 'messages', 'catching', 'process']","['Leftover debug print?', 'Leftover debug print.', 'Leftover debug print.']"
149,29,149_hassconfig_discoveryasync_hassdatasetdefaultdomain_registryasync,"['hassconfig', 'discoveryasync', 'hassdatasetdefaultdomain', 'registryasync', 'async', 'config', 'hass', 'entriesasync', 'hassservicesasync', 'bzucoordinatorasync']","['Use hass.config entries.async setup platforms . L1002-L1005', 'Please use hass.config entries.async setup platforms instead.', 'Here is where I get this error every time I use options flow to add or remove a zone and in the async unload unit test: Even worse, adding and removing zones leaves behind ""ghost"" entities like ""avr"", ""avr1"", ""avr2"" etc that will show as ""unavailable"" but don\'t seem to go away until HA restart. The code is copied from DenonAVR but I must have misunderstood something. The twice is because you remove the ""main"" zone on line 115 with entity registry.async remove(zone.entity id) and again when you call return await hass.config entries.async forward entry unload on line 119. Just remove CONF ZONE1 from the list on line 111. Besides I would first do the await hass.config entries.async forward entry unload , and then remove the left over aditional zones. So move lines 119-121 to 108 If it then still leaves ""ghost"" entities, there is probably something wrong with the unique id matching. easiest way is to add a LOGGER.error() with the zone.unique id on line 113 and then just look in the log how the unique id\'s look like and if they match with your f""{entry.unique id}-{name}""']"
150,29,150_connecting_connect_cannotconnect_connection,"['connecting', 'connect', 'cannotconnect', 'connection', 'connections', 'connectivity', 'disconnect', 'failed', 'error', 'connector']","['Would cannot connect be better?', 'Use cannot connect', 'Please use cannot connect']"
151,29,151_servicevalidationerror_servicevaludationerror_valueerror_servicespy,"['servicevalidationerror', 'servicevaludationerror', 'valueerror', 'servicespy', 'typeerror', 'homeassistanterror', 'error', 'invalid', 'service', 'errors']","['Please use ServiceValidationError instead', 'This too should raise a ServiceValidationError', 'This should raise a ServiceValidationError']"
152,28,152_pop_goes_good_,"['pop', 'goes', 'good', '', '', '', '', '', '', '']","['The same goes for this one.', 'Nothing to pop here', 'Here are a couple of more']"
153,28,153_unused_removed_required_,"['unused', 'removed', 'required', '', '', '', '', '', '', '']","['unused', 'Unused', 'Unused']"
154,27,154_conversationid_conversationasync_conversation_conversationinput,"['conversationid', 'conversationasync', 'conversation', 'conversationinput', 'conversations', 'talk', 'agent', 'converse', 'agentagent', 'agents']","[""Only do this if the conversation agent was previously available for backwards compatibility. I don't think this is necessary because this is the first time Wyoming exposes a conversation agent? This appears to be quite necessary, actually :smile: I had just removed the migration part. Can you explain why you think it is necessary? Creating a conversation entity is the modern way. Setting agents was the old way that we shuoldn't add to new platforms. When I removed it, the agent no longer worked. Should we be calling async set agent automatically in the entity base class? No, that should never be called anymore. It is no longer used. Users should only target the entity ID."", ""Let's call this conversation agent id to be clear what it contains. Did this and renamed command to conversation command everywhere"", 'We should not pass conversation ID to the intent. Conversation ID should be used by the conversation agent to fill in slots based on sentence matching with history, something our default agent does not support.']"
155,27,155_schemaextend_schema_schemas_homeassistantcoreserviceregistryasync,"['schemaextend', 'schema', 'schemas', 'homeassistantcoreserviceregistryasync', 'services', 'service', 'hassservicesasync', 'validator', 'validators', 'validation']","[""We're no longer in a service schema so we shouldn't raise voluptuous errors. Raise a ValueError if we're missing service call parameters. But I think we should think about moving these validations to the service schema. The service schema is defined dynamically and we have access to hass in the schema and all the service data. how do I access hass in the schema validation? I couldn't find any helpers that use hass in helpers config validation . I am all for switching and even potentially doing the device entity ID to node conversion in the validation as well to keep the service logic minimal Define a custom voluptuous validator inside a function that has access to hass , like the method that registers the services. The outer scope will apply to validator without needing to pass the whole scope as parameters. ahh, smart"", ""I'm missing a service validation schema for the services."", ""I'm missing a service validation schema.""]"
156,27,156_stale_code_commented_comment,"['stale', 'code', 'commented', 'comment', 'cleaned', 'refactor', 'async', 'file', 'status', 'switch']","['Stale commented code', 'Stale commented code', 'Stale commented code']"
157,27,157_energy_power_electricity_watts,"['energy', 'power', 'electricity', 'watts', 'kwh', 'watt', 'energyzero', 'smartpower', 'cpower', 'lookinpowerentity']","['All these sensors are using ""Power"" in their name description, but are actually Energy sensors (and thus not Power).', 'To make it clear that it is energy (and not power or something else) done', 'Something is wrong here. Power is not energy, also power should not be TOTAL INCREASING It\'s energy (kw h) even if the device call it ""cpower"". ![image]( Energy is kWh kW h is something completely different. Typo, i wanted to say ""kwh""']"
158,27,158_getter_gettitle_returns_return,"['getter', 'gettitle', 'returns', 'return', 'optional', 'auth', 'default', 'defaults', 'fields', 'field']","['Considering these are optional, .get() could return None . Casting a None with str() returns ""None"" , which is not what you\'ll want for auth ... ... IOW, here and . . . Good catch, resolved', ""user input will always contain the key. No need for get() Thanks. This is my first foray into HA development, learning voluptuous and lovelace has been fun, it's a very slick framework."", 'This could be problematic when .get returns None . Yeah, in theory it never will, because this field always exists, but I did wonder if I should None check it, or not use get.']"
159,27,159_wakeword_wake_wakeworddetectionaborted_wakeworddetectionerror,"['wakeword', 'wake', 'wakeworddetectionaborted', 'wakeworddetectionerror', 'wakeup', 'wakeonlan', 'openwakewordok', 'wol', 'word', 'words0']","['Is this start or end of the wake word ? Or do we just assume that the chunk is just that wake word ?', 'So I am a bit confused here. I thought the goal was that we would try to de-duplicate across both local and Wyoming-based wake words. - Why is it called stt wake up key and not wake word phrase or just wake word ? - Why limit to start stage stt ? That would not work with different styles of wake words. Maybe I\'m being too pedantic by calling it a ""wake-up key"" :smile: But my reasoning was that some wake word models support multiple phrases like ""ok, jarvis"" and ""hey, jarvis"". We don\'t technically know the wake phrase that the user said, just whatever was in the manifest (under wake word ). The different styles (streaming vs local) is a problem. Right now, the wake word system wake word id combo is used as the wake-up key. I think we should change this to the wake word name, which should be something like ""ok nabu"" instead of ok nabu v0.1 and do some normalization so ""hey jarvis"" and ""Hey Jarvis"" still match. We can make rules in our manifest how they should specify wake words. Maybe the property wake word phrase is the most correct here. A single wake word engine could match on multiple wake words. That makes sense. Do you think we should alter the streaming wake word check too? It seems like we\'re moving towards just checking if the ""wake phrases"" (reasonably) match. What do you mean with streaming wake word ? Say you have 2 satellites in the same room, one with local wake word and one using a streaming wake word. Both use ""ok, nabu"". Right now, saying ""ok, nabu"" would wake them both up and duplicate the voice command. Streaming wake word uses the wake word system model name to deconflict (""openWakeWord.ok nabu"") whereas local would just use the wake phrase (""ok nabu""). I\'m proposing these should be considered identical. I agree. We should just look at ok nabu. Using just the wake word phrase now.', 'Should this be wake word too ?']"
160,27,160_flowpytest_tests_testing_initpy,"['flowpytest', 'tests', 'testing', 'initpy', 'test', 'py', 'updatepy', 'init', 'flowpy', 'keyerror']","['Please rename this test module to test init.py .', 'Move these tests to the test config flow.py module.', ""I'd move all the tests to a module test init.py .""]"
161,27,161_mixin_mixins_attribute_sensorstateclassmeasurement,"['mixin', 'mixins', 'attribute', 'sensorstateclassmeasurement', 'optional', 'sensordeviceclasstemperature', 'attributes', 'dataclass', 'intermixed', 'codestyle']","['Please create a mixin so you dont have to set a default', ""Please create a mixin dataclass so we can make this attribute required. See the example I linked. Two questions: 1) Is the purpose of the mixin class to create an abstract placeholder for any future attributes to come? 2) I don't have a use case for exists fn... ? Wouldn't I implement dead code here? Thanks for your help! The purpose of the mixin dataclass is to be able to add an attribute to the entity description that is required. We can't add required attributes in subclasses of dataclass after optional attributes in the parent. The mixin dataclass solves that limitation. I'm not asking you to add the attribute exists fn . The point of the example is to show how to add a mixin dataclass with a required attribute and use the mixin dataclass in the entity description subclass. Done. I hope that works now ..."", 'Mixin already has this']"
162,26,162_dependency_pr_prs_requirementstxt,"['dependency', 'pr', 'prs', 'requirementstxt', 'bump', 'releases', 'bumping', 'superseded', 'bundle', 'include']","[""Does the dependency bump need to go hand in hand with adding the config flow? If not, please move the dependency bump to a separate PR. Yes, the dependency bump is here because we've added the query device info function that is used in the config flow to get the UUID, model name and friendly name and of the TV."", ""This PR is now a dependency bump and a new feature, please split that into 2 separate PR's"", 'Please create a separate PR for the dependency bump']"
163,26,163_yaml_configures_config_configuration,"['yaml', 'configures', 'config', 'configuration', 'platform', 'pr', 'soco', 'changes', 'spi', 'adr0010']","['We no longer allow integrations to add or change a platform YAML configuration. More information on this can be found in Architecture Decision Record: - ADR-0007: decision Please note that this integration connects to a device or service and another Architecture Decision Record applies that disallows the use of YAML configuration in favor of a configuration flow via the UI: - ADR-0010: decision See our developer documentation on how to get started creating a configuration flow for this integration: As these changes often involve a bit of work and some significant shift in the current code, we will close this PR for now. We (and our community!) would really appreciate it if you could start on the refactoring of this integration in a new PR. Thanks already! : 1:', 'We no longer allow integrations to add or change a platform YAML configuration. More information on this can be found in Architecture Decision Record: - ADR-0007: decision Please note that this integration connects to a device or service and another Architecture Decision Record applies that disallows the use of YAML configuration in favor of a configuration flow via the UI: - ADR-0010: decision See our developer documentation on how to get started creating a configuration flow for this integration: As these changes often involve a bit of work and some significant shift in the current code, we will close this PR for now. We (and our community!) would really appreciate it if you could start on the refactoring of this integration in a new PR. Thanks already! : 1:', 'We no longer allow integrations to add or change a platform YAML configuration. More information on this can be found in Architecture Decision Record: - ADR-0007: decision Please note that this integration connects to a device or service and another Architecture Decision Record applies that disallows the use of YAML configuration in favor of a configuration flow via the UI: - ADR-0010: decision See our developer documentation on how to get started creating a configuration flow for this integration: As these changes often involve a bit of work and some significant shift in the current code, we will close this PR for now. We (and our community!) would really appreciate it if you could start on the refactoring of this integration in a new PR. Thanks already! : 1:']"
164,26,164_translatingentityname_entitydescription_entitynaming_nametruemandatoryfornewintegrations,"['translatingentityname', 'entitydescription', 'entitynaming', 'nametruemandatoryfornewintegrations', 'entities', 'entity', 'attributes', 'translatable', 'appendix', 'coordinatorentityravendatacoordinator']","['New integrations entities must support [translation of names and attributes]( Please adopt your code accordingly.', ""For new integrations, we require the following: - Support for [the new entity naming]( has entity name-true-mandatory-for-new-integrations) - Entities and their attributes should be [translatable]( Please adopt your code to add support for it. I think I resolved the new entity naming issue. I'm not sure what to do about making the entities translatable because their names are based on TRIGGERcmd user data. I need to do something, I'll try to figure it out, but I'd appreciate any clues you can give me."", 'For new integrations, we require the following: - Support for [the new entity naming]( has entity name-true-mandatory-for-new-integrations) - Services must be [translatable]( - Entities and their attributes should be [translatable]( Please adopt your code to add support for it. Thanks. Will include it in next commits.']"
165,26,165_registers_register_bytes_byte,"['registers', 'register', 'bytes', 'byte', '4bytes', '16bytes', 'datatype', '0x0100', 'datatypes', '4byte']","['That will not work if count is 1. Work. I add default 2 Should do the same if dont use the data size No it doesn t because you removed the count check, so what happens if I specify count: 5 you need something like register size count ! size', 'The swap is sensitive to the register size, so have a couple of examples, is needed. Maybe the only test needed is ""count 1, size 4"" vs ""count 2, size 2"" All the rest is default I will not support evil stuff above 4 bytes (total byte size) total size count register size', 'But normally register size is 2 (according to standard), when register size is omitted, so it is kind of hard to understand the error message. Apart from that count is valid in combination with register size, so you could have e.g. register size: 4, count 2 Register size: 8, count 1 Count: 4 And they all have the same size.']"
166,26,166_dataclass_dataclasses_dataclassesfields_classes,"['dataclass', 'dataclasses', 'dataclassesfields', 'classes', 'class', 'data', 'object', 'types', 'type', 'defining']","['This could be a dataclass instead.', 'Could be a dataclass', 'This could be a dataclass']"
167,25,167_remove_removed_cleanedup_dont,"['remove', 'removed', 'cleanedup', 'dont', '', '', '', '', '', '']","['Please remove', 'Remove this.', 'Remove']"
168,25,168_tuple_tuples_return_returning,"['tuple', 'tuples', 'return', 'returning', 'items', 'item', 'excluding', 'type', 'backward', 'unpacking']","['Better to not return a tuple though if you can avoid it.', 'This should return a tuple', 'Should return a tuple']"
169,25,169_state_class_missing_package,"['state', 'class', 'missing', 'package', 'model', 'reverted', 'fixed', 'factory', 'itll', 'saving']","['Missing state class', 'Missing state class', 'Missing state class']"
170,24,170_loop_looping_loops_iteration,"['loop', 'looping', 'loops', 'iteration', 'forloop', 'replace', 'moves', 'splitting', 'conditional', 'eliminate']","[""Can you rework this so there is no triple for loop? Should I split this to several functions or should I remove the triple loop? Would a double loop be ok? Something like: tkdrob I simplified the triple loop into a double loop. I hope that's enough, but tell me if you want me to change something else. See aba862e90d13268e4ce68a3f919a15c7908f8e67."", 'We can do this in a loop', 'Use any instead of the loop:']"
171,23,171_constructor_initialize_construction_assign,"['constructor', 'initialize', 'construction', 'assign', 'create', 'entity', 'default', 'property', 'define', 'instances']","['Already set in constructor', 'This can be put in the constructor.', 'This can be put in the constructor. Done']"
172,23,172_code_commented_comment_comments,"['code', 'commented', 'comment', 'comments', 'feedback', 'review', 'reviews', 'explains', 'check', 'missed']","['Commented out code', 'commented out code', 'Commented out code']"
173,23,173_3be9456_8b1d9ba_d949739577125a22a61cd5616584f93bff48e5f3_9a96604fc814869ecb594340ace7a1325230b05b,"['3be9456', '8b1d9ba', 'd949739577125a22a61cd5616584f93bff48e5f3', '9a96604fc814869ecb594340ace7a1325230b05b', '1ee48f0c4f9b892b825ef4b43441bc1d4edded53', 'ed144810a6f7bfef84998ed3747d44fc49e6ff17', '76940724d61cfd07afc5f515f7d90d91acbec322', '872a47ce7183689c9e542c5654d18bdcec515c7e', '38e6f6b57b18eae688f19f737c13f377547b4539', '9843e549a505513ef1029506c2e45f93bdba8060']","['Done in 8b1d9ba', 'Done in [3be9456](', 'Done in [3be9456](']"
174,23,174_coverage_coveragerc_testingyourconfigflow_configflow,"['coverage', 'coveragerc', 'testingyourconfigflow', 'configflow', 'config', 'testing', '100', 'flow', 'configuration', 'tests']","['Config flow needs 100 test coverage', 'You need to have 100 coverage on config flow tests.', 'Config flow should have 100 test coverage']"
175,23,175____,"['', '', '', '', '', '', '', '', '', '']","['Same here', 'Same here', 'Same here']"
176,23,176_conditional_conditionals_ifstatement_ifstatements,"['conditional', 'conditionals', 'ifstatement', 'ifstatements', 'ifs', 'condition', 'return', 'ifcheck', 'nesting', 'nested']","['Does it work for this to be a single statement? (Given this is 3 levels of nested ifs in an inline function) I took advantage of the ability to return early in a successful case to refactor a bit differently. Let me know what you think.', 'Changed if-statement to a guard clause to improve readability.', 'Reverse the conditional here and continue so you can outdent below ok, done']"
177,23,177_stale_comment_comments_explain,"['stale', 'comment', 'comments', 'explain', 'belong', '', '', '', '', '']","['stale comment', 'Stale comment', 'Stale comment?']"
178,22,178_assert_asserting_stateattributesgetoperation_stateentities,"['assert', 'asserting', 'stateattributesgetoperation', 'stateentities', 'state', 'hassstates', 'entity', 'entities', 'checking', 'test']","[""Don't assert details like the coordinator or things in hass.data. Assert state in the state machine."", 'Maybe also assert the entity state?', 'We should assert the state of the entity in the state machine too.']"
179,22,179_entityclassattributes_entityclassorinstanceattributes_attributes_attribute,"['entityclassattributes', 'entityclassorinstanceattributes', 'attributes', 'attribute', 'entity', 'class', 'properties', 'property', 'attr', 'things']","['This can be set as entity class attribute, making the code more compact: entity-class-attributes', 'This can be set as entity class attribute, making the code more compact: entity-class-attributes', 'This can be set as entity class attribute, making the code more compact: entity-class-attributes']"
180,22,180_fixed___,"['fixed', '', '', '', '', '', '', '', '', '']","['Fixed', 'Fixed', 'Fixed']"
181,22,181_used_use_using_anymore,"['used', 'use', 'using', 'anymore', '', '', '', '', '', '']","['Not used', 'Not used', 'Not used.']"
182,21,182_freezer_freeze_fridgefreezer_frozen,"['freezer', 'freeze', 'fridgefreezer', 'frozen', 'freezertick', 'frost', 'using', 'use', 'helper', 'device']","['Please use the freezer', 'Use freezer here too', 'please use the freezer']"
183,21,183_tuya_tuyas_sensor_specification,"['tuya', 'tuyas', 'sensor', 'specification', 'iot', 'specifications', 'device', 'devices', 'datatype', 'data']","[""Attributes are not for debugging. If extra attributes are needed, they can be added in a controlled way (as in specific). Just dumping in all data is not what attributes are for. What data are you trying to expose? .. Frenck frenck in this particular case I will find the valve attribute useful for heating automation, but there are many other useful parameters, such as power state - I will know if the batteries are not discharged in general - I don't see any justification for hiding data. frenck : I've added filtering although as Santanachia mentioned I don't see why hide the data. Depending on the firmware and the settings in the Tuya IoT portal different sets of data can be shown: - standard instruction set (temp set, temp current, mode). This is missing some really usefull data, like valve position, valve setting, etc. - DP instruction set (used eg by Tuya Smart Home application). This set includes much more usefull data, including attributes that I use currently: - fault - to alert on fault - roomtemp calibrat - to automate calibration using another temperature sensor. - valve set (one of normal , ForceOpen , ForceClose ) - allows tracking the valve state - valve - valve open percent. Allows (in conjunction with valve set ) detailed tracing of the Thermostat efficiency - power state - to alert on batteries"", 'How would this work for a motion sensor? As far as I am aware, motion sensors from Tuya are events, and only send the ""on"" state, not the ""off"" state. The Motion sensor within the light sends an off state event based on no motion being detected by the PIR within a defined time, The timings for this are defined in the ""PIR delay"" entity - which in essence defines the time the Light will remain on after motion has stopped being detected. Time can be defined between 5 seconds and 3600 seconds (1 hour) The problem is: Most Tuya devices don\'t send that. Hence it has been removed. Do you mean most Tuya devices in the GYD category or generally? The PIR STATE attribute only seems to be documented against device categories gyd (Motion Sensor Light) and dgnbj (Multi-functional Sensor), the documentation isn\'t that clear, but does suggest that pir state can contain one of two values (""pir"",""none""). The binary sensor does work as noted above against the LEDLIT LTFLPWIFI (reported as PIR-WIFI-2WAY) which I have tested against. Any Pir sensor in Tuya. Regardless of the product type. The implementation details you list, as manufacturer device specific, not Tuya rules standards.', 'I cannot find windows state in the linked documentation. Potentially this is missing from the documentation then as it is returned by the API. Consult the API response: It could be a customization created by the manufacturer, would be nice if there was a specification. In general, we want to avoid adding these. The device works well with Tuya\'s Smart Life application (and within this application these values are exposed), hence I assume that it is intended behavior. This is quite common model I believe, Moes HY360RT TS0601. I\'ve also checked the Tuya IoT portal and all these parameters are there... frenck After some extra investigation: 1. The link refers to Standard Instruction Set only. 2. I\'ve checked the Tuya IoT platform and noticed that the platform allows selecting Control Instruction Mode : - Standard Instruction : ""Control devices with standard instructions after mapping. Various devices across manufacturers can be controlled with a single set of instructions, but note that some functions might not be compatible."" - DP Instruction (it\'s selected in my case, not sure at which point, maybe it was a default): ""Control devices with original DP instructions. You can access any DP information."" The IoT portal suggests both sets are the same (different from [Standard Instruction Set]( DP Instruction Standard Instruction -- -- temp set temp set temp current temp current mode mode child lock child lock fault fault roomtemp calibrat roomtemp calibrat lowtemp lowtemp hightemp hightemp wind wind boost boost valve set valve set comfort temp comfort temp eco temp eco temp valve valve power state power state week state week state Prog Workday Prog Workday Prog Restday Prog Restday Temp holiday Temp holiday windows state windows state Auto Lock Auto Lock Days Holiday Days Holiday 3. I didn\'t get any reply for the tech support ticket. 4. API allows discovering the instruction set supported by the device - [doc]( frenck any update? Note that adding the DP instruction set is rather safe as on the startup, the actually available entities are discovered. I know that there is a performance impact, but this is only for a set of string comparisions. Moreover to fully utilize Tuya Thermostat, it definitely makes sens to enable the DP instruction set.']"
184,21,184_cache_caching_caches_cachetools,"['cache', 'caching', 'caches', 'cachetools', 'cached', 'notcache', 'memory', 'maintain', 'reused', 'invalidate']","[""The light platform caches this value in the init function. Should that be done for this as well? Is there guidance on when to cache or not-cache? It doesn't really matter. I did that because light platform has a bunch of values to watch"", 'why does a write invalidate the cache ?? in many setup the inputs are totally independent from the outputs. Looking at this I think caching will cause a lot more problems. when read is called we want a modbus transaction to take place. Cache shoud guard sequential reads. Each update cycle starts not earlier than 1 second. Since TTL is 1 sec, we can safely remove it to avoid complexity and confusion', 'Should we add the cache decorator since we only need to calculate this once? We need to recalculate it if the WebRTC provider changes, but we will look into it tomorrow. Nick told me we should use cache only on a class or static functions. Otherwise, self will be added to the cache and, therefore, never garbage collected. Also, we can only invalidate the whole cache, not only for one instance. I converted the function to a property so I can use under cached property']"
185,21,185_code_commentedout_commented_copied,"['code', 'commentedout', 'commented', 'copied', '', '', '', '', '', '']","['Commented-out code.', 'Commented code.', 'Commented code.']"
186,20,186_tilt_positioning_vertical_position,"['tilt', 'positioning', 'vertical', 'position', 'cover', 'controls', 'blinds', 'fixed', 'movement', 'covers']","['Please see comments in stop cover tilt Fixed', 'The ATTR TILT POSITION parameter is required.', 'Please see comments in stop cover tilt']"
187,20,187_dhcp_homeassistantcomponentsdhcp_gateway_ip,"['dhcp', 'homeassistantcomponentsdhcp', 'gateway', 'ip', 'ips', 'gateways', 'iwip', 'discovery', 'address', 'discoverable']","[""Is this comment generic for what's needed for any integration's DHCP discovery to work?"", 'You might be able to auto update this via dhcp discovery as well bdraco I did not implement DHCP discovery of the Motion Blinds devices since I don\'t really know how to add DHCP discovery. How can I figure out the required parameters like the OUI of these gateways? Is there some python code I can run that prints all discovered devices? Turn on debug logs for homeassistant.components.dhcp and power cycle the gateway. You should get the mac and hostname in the log. The OUI is just the first 6 of the mac bdraco I just figured out that the OUI of Motion blinds comes from ""Espressif Inc."", I know of 4 gateways all with OUIs from ""Espressif Inc."". Unfortunatley all 4 gateways use a diffrent OUI owned by ""Espressif Inc."" and they own 92 diffrent OUIs, so that is not of much help, that would become a long list.... What does the hostname show up as ? That\'s what registered devices is for. Since you have the mac address in the device entry it will still trigger dhcp flows based on matching that For a Brel blind it shows as ""Iwip"". For a Motion gateway is shows as ""MOTION 6564"" For a separate PR, but something like this should do it The above assumes async handle discovery validates the device is a motion blinds device or aborts bdraco thanks for the example, I will work on this in a seperate PR. Can you merge this PR?', ""See We cannot use Zeroconf (like Somfy integration), since it will only broadcast that service when the device has HomeKit support. Many Overkiz hubs that are supported with this integration don't support HomeKit. dhcp is going to use the default unique id of DOMAIN which means its likely going to get unexpectedly rediscovered. You could implement a async step dhcp that aborts if self. async current entries(include ignore False) is true and otherwise passes to async step user I was waiting for since the gateway id is part of the hostname, thus we could add a async step dhcp that checks if their is a config entry with the same gateway id. I thought other integrations also had this DHCP option, without having the async step dhcp implemented, but happy to take it out here and implement the dhcp with gateway detection in a follow up PR. I'm wrong on this one async handle discovery without unique id actually already handles this case So this good as is. Sorry for leading down the rabbit hole.""]"
188,20,188_abort_abortflow_end_reason,"['abort', 'abortflow', 'end', 'reason', 'stopscript', 'finish', 'test', 'stay', 'shouldnt', 'expect']","['We should add a test for the abort case and entry data update.', 'Check the abort reason too. done in', 'You never abort like this']"
